{
  "title": "Information about: Get all the blog contents",
  "prompt": "Get all the blog contents",
  "sources": [
    "https://nepalprabin.github.io/",
    "https://nepalprabin.github.io/index.html",
    "https://nepalprabin.github.io/projects.html",
    "https://github.com/nepalprabin",
    "https://nepalprabin.github.io/posts/2025-03-02-huggingface-smolagents-solutions.html",
    "https://nepalprabin.github.io/posts/2023-07-04-augmented-language-models.html",
    "https://nepalprabin.github.io/posts/2023-05-15-gpt4-summary.html",
    "https://nepalprabin.github.io/posts/2022-10-19-text-summarization-nlp.html",
    "https://nepalprabin.github.io/posts/2021-10-25-autocorrect-and-minimum-edit-distance.html",
    "https://nepalprabin.github.io/posts/2021-07-27-illustrated-vision-transformers.html",
    "https://nepalprabin.github.io/posts/2021-03-26-simclr-explained.html",
    "https://nepalprabin.github.io/posts/2021-01-01-deep-residual-learning-for-image-recognition-resnet-paper-explained.html",
    "https://nepalprabin.github.io/posts/2020-12-08-self-supervised-learning.html",
    "https://nepalprabin.github.io/posts/2020-09-21-mobilenet-architecture-explained.html",
    "https://nepalprabin.github.io/posts/2020-08-23-neural-style-transfer-and-its-working.html",
    "https://nepalprabin.github.io/posts/2020-08-15-deep-convolutional-general-adversarial-networks-dcgans.html",
    "https://nepalprabin.github.io/posts/2020-08-04-general-adversarial-networks-gans.html",
    "https://nepalprabin.github.io/posts/2020-06-05-paper-explanation-going-deeper-with-convolutions-googlenet.html",
    "https://nepalprabin.github.io/posts/2020-05-09-vggnet-architecture-explained.html"
  ],
  "created_at": "2025-03-05T11:32:23.833044",
  "sections": [
    {
      "title": "Prabin Nepal",
      "url": "https://nepalprabin.github.io/",
      "relevance_score": 0.10391825437545776,
      "content": [
        "Prabin Nepal\nCategories\nAll\n(16)\nLLM\n(1)\nNLP\n(4)\nagents\n(1)\ncomputer-vision\n(6)\ndeep-learning\n(12)\nllms\n(1)\nmachine-learning\n(2)\nself-supervised-learning\n(1)\nHuggingface AI Agents Quiz Solutions\nllms\nagents\nI have been diving into AI agents through Huggingface\u2019s AI Agents Course. This course offers a comprehensive understanding of how to build and deploy AI agents using the\nsmol\u2026\nMar 2, 2025\nAugmenting Large Language Models: Expanding Context and Enhancing Relevance\nmachine-learning\nNLP\ndeep-learning\nLLM\nWith the rise of ChatGPT and other large language models (LLMs), the potential for AI to surpass human capabilities has become a topic of both fascination and concern. While\u2026\nJul 4, 2023\nBrief overview of GPT-4\nmachine-learning\nNLP\ndeep-learning\nSince the release of ChatGPT, there has been significant interest and discussion within the broader AI and natural language processing communities regarding its\u2026\nMar 15, 2023\nText Summarization NLP\nNLP\ndeep-learning\nText summarization is one of the Natural Language Processing (NLP) tasks where documents/texts are shortened automatically while holding the same semantic meaning.\u2026\nOct 19, 2022\nAutocorrect and Minimum Edit Distance\nNLP\ndeep-learning\nThis is my brief note from\nDeepLearning.AI\u2019s\nNLP Specialization Course.\nOct 25, 2021\nIllustrated Vision Transformers\ncomputer-vision\ndeep-learning\nEver since Transformer was introduced in 2017, there has been a huge success in the field of Natural Language Processing (NLP). Almost all NLP tasks use Transformers and\u2026\nJul 27, 2021\nPaper Explanation: A Simple Framework for Contrastive Learning of Visual Representations (simCLR)\nVarious self-supervised learning methods have been proposed in recent years for learning image representations. Though a lot of methods have been proposed, the performance\u2026\nMar 26, 2021\nDeep Residual Learning for Image Recognition (ResNet paper explained)\nDeep Neural Networks tend to provide more accuracy as the number of layers increases. But, as we go more deeper in the network, the accuracy of the network decreases instead\u2026\nJan 1, 2021\nSelf-supervised Learning\ndeep-learning\nself-supervised-learning\nI have been exploring self-supervised learning and been through papers and blogs to understand it. Self-supervised learning is considered the next big thing in deep learning\u2026\nDec 8, 2020\nMobileNet Architecture Explained\ndeep-learning\nIn this blog post, I will try to write about the MobileNets and its architecture. MobileNet uses depthwise separable convolutions instead of standard convolution to reduce\u2026\nSep 21, 2020\nNeural style transfer and its working\nHave you ever used an app called Prisma that styles your image using popular paintings and turns your photo stunning? If that\u2019s the case then, the app you are using is the\u2026\nAug 23, 2020\nDeep Convolutional Generative Adversarial Networks (DCGANs)\ncomputer-vision\ndeep-learning\nDCGAN (Deep Convolutional General Adversarial Networks) uses convolutional layers in its design.\nAug 15, 2020\nGeneral Adversarial Networks (GANs)\ncomputer-vision\ndeep-learning\n\u201c\nGeneral Adversarial Nets\nis the most interesting idea in the last 10 years in machine learning\u201d. This was the statement from Yann LeCun regarding GANs when Ian Goodfellow\u2026\nAug 4, 2020\nPaper Explanation: Going deeper with Convolutions (GoogLeNet)\ncomputer-vision\ndeep-learning\nGoogle proposed a deep Convolution Neural Network named inception that achieved top results for classification and detection in ILSVRC 2014.\nJun 5, 2020\nVGGNet Architecture Explained\ncomputer-vision\ndeep-learning\nVGGNet is a Convolutional Neural Network architecture proposed by Karen Simonyan and Andrew Zisserman of University of Oxford in 2014. This paper mailny focuses in the\u2026\nMay 9, 2020\nAlexNet Architecture Explained\ncomputer-vision\ndeep-learning\nAlexNet famously won the 2012 ImageNet LSVRC-2012 competition by a large margin (15.3% vs 26.2%(second place) error rates). Here is the link to original paper.\nApr 24, 2020\nNo matching items"
      ]
    },
    {
      "title": "Prabin Nepal",
      "url": "https://nepalprabin.github.io/index.html",
      "relevance_score": 0.10391825437545776,
      "content": []
    },
    {
      "title": "Projects \u2013 Prabin Nepal",
      "url": "https://nepalprabin.github.io/projects.html",
      "relevance_score": 0.04149121046066284,
      "content": [
        "Projects \u2013 Prabin Nepal\n1.\nStanford NLP Lecture Transcription using OpenAI\u2019s Whisper\nWhisper is an automatic speech recognition (ASR) model trained on hours of multilingual and multitask supervised data. It is implemented as an encoder-decoder transformer architecture where audio are splitted into 30 seconds of chunks, converted into a log-Mel spectrogram, and then passed into an encoder. The decoder is trained to predict the corresponding text caption, intermixed with special tokens that direct the single model to perform tasks such as language identification, phrase-level timestamps, multilingual speech transcription, and to-English speech translation. For more info about whisper, read\nhere\n.\nI used whisper model to transcribe Stanford NLP lectures into corresponding text captions.\nHere\nis the result of the transcribed lectures. This web app is build using Flask and deployed on AWS EC2 instance. You can find transcribed audio file in the form of text\nhere\n.\n2.\nCustom Named Entity Recognizer for clinical data\nNamed Entity Recognition (NER) is a subtask of Natural Language Processing (NLP) that involves identifying and categorizing named entities in text.\nI have developed a custom named entity recognition (NER) model for clinical data using the spacy framework and deployed it using Streamlit. The model is capable of identifying various entities such as diseases, treatments, medications, and anatomical locations from clinical text data. The model classifies entities based on three classes:\n\u2018MEDICINE\u2019\n,\n\u201cMEDICALCONDITION\u201d\n, and\n\u201cPATHOGEN\u201d\n. The dataset was used from\nkaggle\n. You can try the application on this\nlink\n3.\nQuestion Answering using Langchain and OpenAI\nThis application provides a simple example of how to build a question-answering system using Langchain and pre-trained language models from OpenAI and Streamlit.\nLangchain helps to build Large Language Models (LLMs) through composability. It helps to combine large language models with other sources of computation.\nI developed a question answering system using Langchain with OpenAI embeddings. Since, LLMs tends to have fixed context length, Langchain helps to eliminate this issue by introducing chains, where we can break the document into different chunks and run the chain on the whole document. In this application, when a user uploads a file, the contents are converted into embeddings using OpenAI embeddings and stored in Pinecone vector database. Storing embeddings this way, helps for faster retrieval of the embeddings. When a user enters the query, similarity search is conducted to retrieve the similar embeddings from the vector store and the langchain chain passes the formatted response to the LLM."
      ]
    },
    {
      "title": "nepalprabin (Prabin Nepal) \u00b7 GitHub",
      "url": "https://github.com/nepalprabin",
      "relevance_score": 0.12817977368831635,
      "content": [
        "nepalprabin (Prabin Nepal) \u00b7 GitHub\nSkip to content\nYou signed in with another tab or window.\nReload\nto refresh your session.\nYou signed out in another tab or window.\nReload\nto refresh your session.\nYou switched accounts on another tab or window.\nReload\nto refresh your session.\nDismiss alert\nnepalprabin\nFollow\nMore\nOverview\nRepositories\nProjects\nPackages\nStars\nnepalprabin\nFollow\n\ud83c\udfaf\nFocusing\nPrabin Nepal\nnepalprabin\n\ud83c\udfaf\nFocusing\nFollow\nSoftware Developer. AI enthusiast\n5\nfollowers\n\u00b7\n11\nfollowing\nUSA\nAchievements\nx2\nAchievements\nx2\nBlock or Report\nBlock or report nepalprabin\nBlock user\nPrevent this user from interacting with your repositories and sending you notifications.\nLearn more about\nblocking users\n.\nYou must be logged in to block users.\nAdd an optional note:\nPlease don't include any personal information such as legal names or email addresses. Maximum 100 characters, markdown supported. This note will be visible to only you.\nBlock user\nReport abuse\nContact GitHub support about this user\u2019s behavior.\nLearn more about\nreporting abuse\n.\nReport abuse\nMore\nOverview\nRepositories\nProjects\nPackages\nStars\nnepalprabin\n/\nREADME\n.md\nHi \ud83d\udc4b, I'm Prabin Nepal\nA passionate software developer\n\ud83c\udf31 I\u2019m currently learning\nDeep Learning for Computer Vision and NLP\n\ud83d\udcac Ask me about\nFull Stack Development, Deep Learning\n\ud83d\udceb How to reach me\nprabinnepal1996@gmail.com\nPinned\nLoading\noswrite\noswrite\nPublic\nPython\n2\nbasic-deeplearning-notebooks\nbasic-deeplearning-notebooks\nPublic\nJupyter Notebook\ndeeplearning-paper-implementation\ndeeplearning-paper-implementation\nPublic\nJupyter Notebook\n3\n1\nwhisper-webapp\nwhisper-webapp\nPublic\nJupyter Notebook\nSomething went wrong, please refresh the page to try again.\nIf the problem persists, check the\nGitHub status page\nor\ncontact support\n.\nYou can\u2019t perform that action at this time."
      ]
    },
    {
      "title": "Huggingface AI Agents Quiz Solutions \u2013 Prabin Nepal",
      "url": "https://nepalprabin.github.io/posts/2025-03-02-huggingface-smolagents-solutions.html",
      "relevance_score": 0.11768277734518051,
      "content": [
        "Huggingface AI Agents Quiz Solutions \u2013 Prabin Nepal\nI have been diving into AI agents through Huggingface\u2019s AI Agents Course. This course offers a comprehensive understanding of how to build and deploy AI agents using the\nsmolagents\nlibrary. In this blog, I\u2019ll share insights from the course (Unit 2) and provide code snippets to illustrate key concepts.\nNote\nHere is the course link if anyone is interested.\nAI Agents Course\nCreate a Basic Code Agent with Web Search Capability\nOne of the foundational exercises involves creating a CodeAgent equipped with web search capabilities. This agent leverages the DuckDuckGoSearchTool to perform web searches, enabling it to fetch real-time information. Here\u2019s how you can set it up:\n# Create a CodeAgent with DuckDuckGo search capability\nfrom\nsmolagents\nimport\nCodeAgent, DuckDuckGoSearchTool, HfApiModel\nagent\n=\nCodeAgent(\ntools\n=\n[DuckDuckGoSearchTool()],\n# Add search tool here\nmodel\n=\nHfApiModel(\n\"Qwen/Qwen2.5-Coder-32B-Instruct\"\n)\n# Add model here\n)\nIn this snippet, we initialize a CodeAgent with the DuckDuckGoSearchTool, allowing the agent to perform web searches to answer queries.\nSet Up a Multi-Agent System with Manager and Web Search Agents\nMulti-Agent systems are the agents that are specialized on complex tasks with more scalable and robust nature. In\nsmolagents\n, various agents can be integrated to produce Python code, invoke external tools, conduct web searches, and more. By coordinating these agents, it\u2019s possible to develop robust workflows. A typical multi-agent system includes:\n- A manager Agent\n- A code interpreter Agent\n- A web Search Agent\nMulti-agent system allows to separate memories between different sub-tasks and provide great benefits. Firstly, each agent are more focused on its core taks and secondly, separating memories reduces the count of input tokens resulting in reducing latency and cost. Below is the multi-agent system when\nweb_agent\nperforms search and\nmanager_agent\ngives data analysis capabilities. Also, we can import dependencies (like python libraries) that helps to perform the tasks.\nfrom\nsmolagents\nimport\nCodeAgent, ToolCallingAgent, DuckDuckGoSearchTool, HfApiModel, VisitWebpageTool\nweb_agent\n=\nToolCallingAgent(\ntools\n=\n[DuckDuckGoSearchTool(), VisitWebpageTool()],\nmodel\n=\nHfApiModel(model_id\n=\n\"Qwen/Qwen2.5-Coder-32B-Instruct\"\n),\nmax_steps\n=\n10\n,\nname\n=\n\"search\"\n,\ndescription\n=\n\"Agent to perform web searches and visit webpages.\"\n)\nmanager_agent\n=\nCodeAgent(\nmodel\n=\nHfApiModel(model_id\n=\n\"Qwen/Qwen2.5-Coder-32B-Instruct\"\n),\nmanaged_agents\n=\n[web_agent],\nadditional_authorized_imports\n=\n[\n\"pandas\"\n,\n\"time\"\n,\n\"numpy\"\n]\n# Corrected imports\n)\nConfigure Agent Security Settings\nSecurity is a crucial aspect when deploying AI agents, especially when they execute code. Below code snippet uses E2B to run code in a sandboxed environment. It is a remote execution that run the code in a isolated container.\nfrom\nsmolagents\nimport\nCodeAgent, HfApiModel\nfrom\nsmolagents.sandbox\nimport\nE2BSandbox\nmodel\n=\nHfApiModel(\n\"Qwen/Qwen2.5-Coder-32B-Instruct\"\n)\nagent\n=\nCodeAgent(\ntools\n=\n[],\nmodel\n=\nmodel,\nsandbox\n=\nE2BSandbox(),\n# Configure the sandbox\nadditional_authorized_imports\n=\n[\n\"numpy\"\n],\n# Authorize numpy import\n)\nImplement a Tool-Calling Agent\nSimilar to\nCodeAgent\n,\nToolCallingAgent\nis another type of agent available in smolagent library. CodeAgent uses Python code snippets whereas ToolCallingAgent use built-in tool-calling capabilities of LLM providers and generate JSON structures.\nfrom\nsmolagents\nimport\nToolCallingAgent, HfApiModel, DuckDuckGoSearchTool\nagent\n=\nToolCallingAgent(\ntools\n=\n[DuckDuckGoSearchTool()],\nmodel\n=\nHfApiModel(model_id\n=\n\"Qwen/Qwen2.5-Coder-32B-Instruct\"\n),\nname\n=\n\"SearchAgent\"\n,\ndescription\n=\n\"An agent that uses DuckDuckGo to search the web.\"\n,\nmax_steps\n=\n5\n,\n)\nSet Up Model Integration\nLLM models are the most important aspect when creating AI agents. There are many model availables for various tasks and domains. So we can easily integrate models that is required for our task. Below code snippet switches between two different models providers.\nfrom\nsmolagents\nimport\nHfApiModel, LiteLLMModel\n# Initialize Hugging Face model\nhf_model\n=\nHfApiModel(model_id\n=\n\"Qwen/Qwen2.5-Coder-32B-Instruct\"\n)\n# Initialize LiteLLM model as an alternative model\nother_model\n=\nLiteLLMModel(model_id\n=\n\"anthropic/claude-3-sonnet\"\n)\n# Set the model to hf_model or alternative model\nmodel\n=\nhf_model\n# Alternatively, you can switch this to `other_model`"
      ]
    },
    {
      "title": "Augmenting Large Language Models: Expanding Context and Enhancing Relevance \u2013 Prabin Nepal",
      "url": "https://nepalprabin.github.io/posts/2023-07-04-augmented-language-models.html",
      "relevance_score": 0.1302972435951233,
      "content": [
        "Augmenting Large Language Models: Expanding Context and Enhancing Relevance \u2013 Prabin Nepal\nWith the rise of ChatGPT and other large language models (LLMs), the potential for AI to surpass human capabilities has become a topic of both fascination and concern. While LLMs excel at understanding language, following instructions, and reasoning, they often fall short when it comes to performing specific tasks. Simply inputting a prompt into ChatGPT may result in answers that are unrelated or out of context, a phenomenon known as \u201challucination.\u201d To obtain relevant information, it is crucial to provide the model with the appropriate context. However, the size of the context window is limited, posing a challenge in capturing all necessary information. Although the context size has increased over time, storing extensive information within a fixed context window remains impractical and expensive. This is where the augmentation of language models comes into play.\nAugmenting large language models involves three primary approaches:\nretrieval,\nchains, and\ntools.\nThese methods aim to enhance the capabilities of LLMs by providing them with additional resources and functionalities.\nRetrieval Augmentation:\nRetrieval augmentation involves leveraging an external corpus of data for the language model to search through. Traditionally, retrieval algorithms employ queries to rank relevant objects in a collection, which can include images, texts, documents, or other types of data. To enable efficient searching, the documents and their corresponding features are organized within an index. This index maps each feature to the documents containing it, facilitating quick retrieval. Boolean search determines the relevance of documents based on the query, while ranking is typically performed using algorithms like BM25 (Best Match 25).\nBM25 (Best Match 25) is a ranking function commonly used in information retrieval to measure the relevance of a document to a given query. It is a probabilistic retrieval model that enhances the vector space model by incorporating document length normalization and term frequency saturation.\nIn BM25, the indexing process involves tokenizing each document in the collection into terms and calculating term statistics such as document frequency (df) and inverse document frequency (idf). Document frequency represents the number of documents in the collection containing a particular term, while inverse document frequency measures the rarity of the term across the collection.\nDuring the querying phase, the query is tokenized into terms, and term statistics, including query term frequency (qtf) and query term inverse document frequency (qidf), are computed. These statistics capture the occurrence and relevance of terms in the query.\nWhile traditional retrieval methods primarily rely on keyword matching and statistical techniques, modern approaches leverage AI-centric retrieval methods that utilize embeddings. These methods offer improved search capabilities and help retrieve contextually relevant information.\nChains\nChains involve using the output of one language model as the input for another. By cascading multiple models together, the output of each model becomes the input for the subsequent one. This chaining process allows the models to build upon each other\u2019s knowledge and reasoning abilities, potentially leading to more accurate and contextually appropriate responses.\nThe sequential arrangement of models in a chain creates a pipeline of of interconnected language models, where the output of one model serves as the input for the next. This pipeline allows for a cascading flow of information and reasoning, enabling the models to collectively enhance their understanding and generate more accurate responses. By leveraging a chain of language models, each model can contribute its specialized knowledge and capabilities to the overall task. For example, one model may excel at language comprehension, while another may possess domain-specific knowledge.\nAs the input passes through the chain, each model can refine and expand upon the information, leading to a more comprehensive and contextually relevant output. The chaining process in language models has the potential to address the limitations of individual models, such as hallucination or generating irrelevant responses. By combining the strengths of multiple models, the pipeline can help mitigate these issues and produce more reliable and accurate results.\nFurthermore, the pipeline can be customized and tailored to specific use cases or tasks. Different models can be integrated into the chain based on their strengths and compatibility with the desired objectives. This flexibility allows for the creation of powerful and specialized systems that leverage the collective intelligence of multiple language models.\nLangchain\nLangchain\nhas emerged as an immensely popular tool for constructing chains of language models, making it one of the fastest-growing open-source projects in this domain. With support for both Python and JavaScript, it provides a versatile platform for building applications and can be seamlessly integrated into production environments. Langchain serves as the fastest way to kickstart development and offers a wide range of pre-built chains tailored for various tasks. Many developers find inspiration from Langchain and end up creating their own customized chaining solutions. One of the key strengths of Lang chain lies in its extensive repository, which houses numerous examples of different chaining patterns. These examples not only facilitate idea generation but also serve as valuable resources for learning and gaining insights into effective chaining techniques. Whether for rapid prototyping or constructing production-grade systems, Lang chain strikes a balance between ease of use and flexibility, empowering developers to effortlessly create their own chaining systems when needed.\nThe building block of Langchain are chains. Chains can be simple/generic or specialized. One simple chain is a generic chain that contains a single LLM. Generic chain takes a prompt and uses LLM for text generation based on the prompt. Let\u2019s see how to achieve a simple chain using OpenAI\u2019s gpt-3.5 turbo model.\nimport\nos\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"...\"\nfrom\nlangchain.prompts\nimport\nPromptTemplate\ntemplate\n=\n\"\"\"\nWho won the oscar for the best actor in  a leading role on\n{year}\n?\n\"\"\"\nprompt\n=\nPromptTemplate(\ninput_variables\n=\n[\n\"year\"\n],\ntemplate\n=\ntemplate,\n)\nprint\n(prompt.\nformat\n(year\n=\n2012\n))\nOutput: Who won the oscar\nfor\nthe best actor\nin\na leading role on\n2012\n?\nPromptTemplate\nhelps to design prompt for your tasks and you can provide input variables if you want like below:\ntemplate\n=\n\"\"\"\nWho won the oscar for the best\n{role}\non\n{year}\n?\n\"\"\"\nWhile creating a prompt template for multiple variables, you need to pass all those variables in\ninput_variables\nargument\nprompt\n=\nPromptTemplate(\ninput_variables\n=\n[\n\"role\"\n,\n\"year\"\n],\ntemplate\n=\ntemplate,\n)\nTools\nThe another way to give LLMs access to outside world is to let them use tools.\nUsing Tools in Langchain\nTools are the flexible way to augment language model with external data. There are two ways to build tools into language models. First way is to manually create chains whereas the later one is the use of plugins and letting the model figure it out. Some example tools that can be use includes Arxiv, Bash, Bing Search, Google, etc.\nTools can be used in langchain using following code snippet (in Python):\nfrom\nlangchain.agents\nimport\nload_tools\ntool_names\n=\n[...]\ntools\n=\nload_tools(tool_names)\ntools\nYou can name the tools that you are going to use and load them using load_tools methods\nLet\u2019s use Python\u2019s requests module as a tool to extract data from the web\nfrom\nlangchain.agents\nimport\nload_tools\ntool_names\n=\n[\n'requests_all'\n]\nrequests_tools\n=\nload_tools(tool_names)\nrequests_tools\nOutput:\n[\nRequestsGetTool(name\n=\n'requests_get'\n, description\n=\n'A portal to the internet. Use this when you need to get specific content from a website. Input should be a  url (i.e. https://www.google.com). The output will be the text response of the GET request.'\n, args_schema\n=\nNone\n, return_direct\n=\nFalse\n, verbose\n=\nFalse\n, callbacks\n=\nNone\n, callback_manager\n=\nNone\n, handle_tool_error\n=\nFalse\n, requests_wrapper\n=\nTextRequestsWrapper(headers\n=\nNone\n, aiosession\n=\nNone\n)),\nRequestsPostTool(name\n=\n'requests_post'\n, description\n=\n'Use this when you want to POST to a website.\n\\n\nInput should be a json string with two keys: \"url\" and \"data\".\n\\n\nThe value of \"url\" should be a string, and the value of \"data\" should be a dictionary of\n\\n\nkey-value pairs you want to POST to the url.\n\\n\nBe careful to always use double quotes for strings in the json string\n\\n\nThe output will be the text response of the POST request.\n\\n\n'\n, args_schema\n=\nNone\n, return_direct\n=\nFalse\n, verbose\n=\nFalse\n, callbacks\n=\nNone\n, callback_manager\n=\nNone\n, handle_tool_error\n=\nFalse\n, requests_wrapper\n=\nTextRequestsWrapper(headers\n=\nNone\n, aiosession\n=\nNone\n)),\nRequestsPatchTool(name\n=\n'requests_patch'\n, description\n=\n'Use this when you want to PATCH to a website.\n\\n\nInput should be a json string with two keys: \"url\" and \"data\".\n\\n\nThe value of \"url\" should be a string, and the value of \"data\" should be a dictionary of\n\\n\nkey-value pairs you want to PATCH to the url.\n\\n\nBe careful to always use double quotes for strings in the json string\n\\n\nThe output will be the text response of the PATCH request.\n\\n\n'\n, args_schema\n=\nNone\n, return_direct\n=\nFalse\n, verbose\n=\nFalse\n, callbacks\n=\nNone\n, callback_manager\n=\nNone\n, handle_tool_error\n=\nFalse\n, requests_wrapper\n=\nTextRequestsWrapper(headers\n=\nNone\n, aiosession\n=\nNone\n)),\nRequestsPutTool(name\n=\n'requests_put'\n, description\n=\n'Use this when you want to PUT to a website.\n\\n\nInput should be a json string with two keys: \"url\" and \"data\".\n\\n\nThe value of \"url\" should be a string, and the value of \"data\" should be a dictionary of\n\\n\nkey-value pairs you want to PUT to the url.\n\\n\nBe careful to always use double quotes for strings in the json string.\n\\n\nThe output will be the text response of the PUT request.\n\\n\n'\n, args_schema\n=\nNone\n, return_direct\n=\nFalse\n, verbose\n=\nFalse\n, callbacks\n=\nNone\n, callback_manager\n=\nNone\n, handle_tool_error\n=\nFalse\n, requests_wrapper\n=\nTextRequestsWrapper(headers\n=\nNone\n, aiosession\n=\nNone\n)),\nRequestsDeleteTool(name\n=\n'requests_delete'\n, description\n=\n'A portal to the internet. Use this when you need to make a DELETE request to a URL. Input should be a specific url, and the output will be the text response of the DELETE request.'\n, args_schema\n=\nNone\n, return_direct\n=\nFalse\n, verbose\n=\nFalse\n, callbacks\n=\nNone\n, callback_manager\n=\nNone\n, handle_tool_error\n=\nFalse\n, requests_wrapper\n=\nTextRequestsWrapper(headers\n=\nNone\n, aiosession\n=\nNone\n))\n]\nEach tool inside the\nrequest_all\ntool contains a request wapper. We can directly work with these wrappers as below:\nrequests_tools[\n0\n].requests_wrapper\nOutput:\nTextRequestsWrapper(headers\n=\nNone\n, aiosession\n=\nNone\n)\nWe can use\nTextRequestsWrapper\nto create a request object and use the object to extract data from the web.\nfrom\nlangchain.utilities\nimport\nTextRequestsWrapper\nrequests\n=\nTextRequestsWrapper()\nrequests.get(\n\"https://reqres.in/api/users?page=2\"\n)\nOutput:\n'{\"page\":2,\"per_page\":6,\"total\":12,\"total_pages\":2,\"data\":[{\"id\":7,\"email\":\"michael.lawson@reqres.in\",\"first_name\":\"Michael\",\"last_name\":\"Lawson\",\"avatar\":\"https://reqres.in/img/faces/7-image.jpg\"},{\"id\":8,\"email\":\"lindsay.ferguson@reqres.in\",\"first_name\":\"Lindsay\",\"last_name\":\"Ferguson\",\"avatar\":\"https://reqres.in/img/faces/8-image.jpg\"},{\"id\":9,\"email\":\"tobias.funke@reqres.in\",\"first_name\":\"Tobias\",\"last_name\":\"Funke\",\"avatar\":\"https://reqres.in/img/faces/9-image.jpg\"},{\"id\":10,\"email\":\"byron.fields@reqres.in\",\"first_name\":\"Byron\",\"last_name\":\"Fields\",\"avatar\":\"https://reqres.in/img/faces/10-image.jpg\"},{\"id\":11,\"email\":\"george.edwards@reqres.in\",\"first_name\":\"George\",\"last_name\":\"Edwards\",\"avatar\":\"https://reqres.in/img/faces/11-image.jpg\"},{\"id\":12,\"email\":\"rachel.howell@reqres.in\",\"first_name\":\"Rachel\",\"last_name\":\"Howell\",\"avatar\":\"https://reqres.in/img/faces/12-image.jpg\"}],\"support\":{\"url\":\"https://reqres.in/#support-heading\",\"text\":\"To keep ReqRes free, contributions towards server costs are appreciated!\"}}'\nReferences\nFull Stack Deep Learning (LLM Bootcamp)\nLangchain"
      ]
    },
    {
      "title": "Brief overview of GPT-4 \u2013 Prabin Nepal",
      "url": "https://nepalprabin.github.io/posts/2023-05-15-gpt4-summary.html",
      "relevance_score": 0.040627866983413696,
      "content": [
        "Brief overview of GPT-4 \u2013 Prabin Nepal\nSince the release of ChatGPT, there has been significant interest and discussion within the broader AI and natural language processing communities regarding its capabilities. In addition to this, ChatGPT has captured the attention of the internet at large due to its remarkable ability to generate fluent and natural-sounding responses across a wide range of prompts and language tasks. Due to this, it became fastest growing consumer application in the history, just two months after the launch. ChatGPT is fine-tuned from a model in the GPT-3.5 series and can write articles, jokes, poetrys in response to the prompt. Though powerful, there have also been concerns raised about the potential risks associated with it and other large language models (LLMs), particularly with respect to issues such as bias, and misinformation. One of the major concern for LLMs is that it suffers from\nhallucination\n.\nHallucination refers to the phenomenon where the model generates responses that are not supported by the input or are inconsistent with reality. This can happen when the model generates text that appears to be coherent and relevant, but is not grounded in any factual or contextual information.\nA year after releasing ChatGPT, OpenAI released GPT-4 (on 14th March, an improved version of GPT-3.5 model that supports multimodal data. It is capable of processing text and image data to generate textual data. It achieved human level performance on various professional and academic benchmarks. On a simulated bar exam, GPT-4 achieved a score that falls on the top 10% of the exam takes. In contrast, the score achieved by previous model GPT-3.5 fell on bottom 10%. This shows the level of improvement achieved by the latest version of GPT. It is also important to mention that the model was not specifically trained on these exams. A minority of problems were seen by model while training.\nCapabilities of GPT-4\nThough the report does not provide any details about architecture (including model size), hardware, training compute, dataset construction, or training method, a demo run by\nGreg Brockman\n(President and Co-founder, OpenAI) after the release of GPT-4 shows various capabilities of the model.\nYou can watch the GPT-4 Developer Livestream replay here:\n1. Supports longer context\nGPT-4 is capable of handling over 25,000 words of text, that enables its usage in situations that require the creation of lengthy content, extended dialogues, or the exploration and analysis of extensive documents.\n2. Hand-drawn pencil drawing turned into a fully functional website\nGPT-4 is also capable of handling visual input, such as hand-drawn pencil drawings that looks like a mock design, and generating code to create a website. The generated output is mind blowing. Another important aspect is the accuracy by which the model is able to perform OCR task with such messy handwritings.\nFig. Left is the mock design and right is the website created using the code generated from gpt4-model.\nsource\n3. GPT-4 can describe the image.\nAs opposed to text on prompts (on previous GPT version), this model accepts inputs containing both text and images. It lets user specify any language or vision tasks. GPT-4 displays comparable skills on various types of content, such as documents containing both textual and visual elements like photographs, diagrams, or screenshots, as it does when dealing with text-only inputs.\nExample prompt demonstrating GPT-4\u2019s visual input capability. The prompt consists of a question about an image with multiple panels which GPT-4 is able to answer.\nsource\n4. Human level performance on professional and academic benchmarks\nGPT outperforms the previous state-of-the-art models on various standardized exams, such as GRE, SAT, BAR, and APs, along with other research benchmarks like MMLU, HellaSWAG, and TextQA. GPT-4 outperforms the English language performance of GPT 3.5 and existing language models (\nChinchilla\nand\nPaLM\n), including low-resource languages such as Latvian, Welsh, and Swahili.\nLimitations of GPT-4\nThough there has been a tremendous improvement as compared to previous models, GPT-4 has similar limitations as earlier GPT models. It is not fully reliable and hallucinates.\nSince GPT-4 is trained on the data available till September 2021, it lacks knowledge of the events occured after that time period.\nRisks and mitigations\nThe prompts entered by the users are not always safe. When providing unsafe inputs to the model, it may generate undesirable text like commiting crimes. To mitigate these risks, various approaches like Adversarial Testing, Model Assisted Safety Pipeline are carried out. Using domain experts and their findings, model is improved to refuse request for unsafe inputs like synthesizing dangerous chemicals.\nExamples of how unsafe inputs are refused by the model\nConclusion\nThe recent advancements in GPT-4, have proven to outperform existing language models in a collection of NLP tasks. The improved capabilities of GPT-4 are not limited to the English language, as predictable scaling allows for accurate predictions in many different languages. However, the increased capabilities of GPT-4 also present new risks, which require significant work to understand and improve its safety and alignment. Nevertheless, GPT-4 marks a significant milestone towards the development of broadly useful and safely deployed AI systems.\nReferences:\nGPT-4 Technical Report\nGPT-4 Blog Post\nchat.openai.com\nPS\nWhile GPT-4 may have stolen the headlines, it was not the only new technology on display. AnthropicAI unveiled\nClaude\n, next gen AI assistant can help with use cases including summarization, search, creative and collaborative writing, Q&A, coding, and more. Meanwhile, Google AI released\nPaLM\n, an entry point for Google\u2019s large language models with variety of applications. With these three new systems, the future of AI looks brighter than ever before."
      ]
    },
    {
      "title": "Text Summarization NLP \u2013 Prabin Nepal",
      "url": "https://nepalprabin.github.io/posts/2022-10-19-text-summarization-nlp.html",
      "relevance_score": 0.12227651476860046,
      "content": [
        "Text Summarization NLP \u2013 Prabin Nepal\nWhat is text summarization?\nText summarization is one of the Natural Language Processing (NLP) tasks where documents/texts are shortened automatically while holding the same semantic meaning. Summarization process generates short, fluent and accurate summary of the long documents. The main idea of text summarization is to find the subset of the most important information from the entire document and present it in a human readable format. Text summarization has its application in other NLP tasks such as Question Answering (QA), Text Classification, Text Generation and other fields.\nTypes of summarization\nBased on how the texts are extracted from the documents, the summarization process can be divided into two types: extractive summarization and abstractive summarization.\nExtractive Summarization\nExtractive summarization picks up the most important sentences directly from the documents and forms a coherent summary. This is done using a scoring function. Extractive summarization takes a sentence as an input and produces a probability vector as the output. This probability vector represents the probability of a sentence being included in the summary.\nImplementing extractive summarization based on word frequency\nWe can implement extractive summarization using word frequency in five simple steps:\na. Creating word frequency table\nWe count the frequency of the words present in the text and create a frequency table which is a dictionary to store the count. While creating the frequency table, we do not account for the stop words present in the text and remove those words.\ndef\nfrequency_table(text):\n# all unique stopwords of english\nstop_words\n=\nset\n(stopwords.words(\n\"english\"\n))\nwords\n=\nword_tokenize(text)\nfreq_table\n=\ndict\n()\n# creating frequency table to keep the count of each word\nfor\nword\nin\nwords:\nword\n=\nword.lower()\nif\nword\nin\nstop_words:\ncontinue\nif\nword\nin\nfreq_table:\nfreq_table[word]\n+=\n1\nelse\n:\nfreq_table[word]\n=\n1\nreturn\nfreq_table\nb. Tokenizing the sentences\nHere we tokenize the sentences using NLTK\u2019s sent_tokenize() method. This separates paragraphs into individual sentences.\ndef\ntokenize_sentence(text):\nreturn\nsent_tokenize(text)\nc.\u00a0Scoring the sentences using term frequency\nHere, we score a sentence by its words, by adding frequency of every word present in the sentence excluding stop words. One downside of this approach is, if the sentence is long, the value of frequency increases.\ndef\nterm_frequency_score(sentence, freq_table):\n# dictionary to keep the score\nsentence_value\n=\ndict\n()\nfor\nsentence\nin\nsentences:\nfor\nword, freq\nin\nfreq_table.items():\nif\nword\nin\nsentence.lower():\nif\nsentence\nin\nsentence_value:\nsentence_value[sentence]\n+=\nfreq\nelse\n:\nsentence_value[sentence]\n=\nfreq\nreturn\nsentence_value\nd.\u00a0Finding the threshold score\nAfter calculating the term frequency, we calculate the threshold score.\ndef\ncalculate_average_score(sentence_value):\n# To compare the sentences within the text, we assign a score.\nsum_values\n=\n0\nfor\nsentence\nin\nsentence_value:\nsum_values\n+=\nsentence_value[sentence]\n# Calculating average score of the sentence. This average score can be a good threshold.\naverage\n=\nint\n(sum_values\n/\nlen\n(sentence_value))\nreturn\naverage\ne. Generating the summary based on the threshold value\nBased on the threshold value, we generate the summary of the text.\ndef\ncreate_summary(sentences, sentence_value, threshold):\n# Applying the threshold value and storing sentences in an order into the summary.\nsummary\n=\n''\nfor\nsentence\nin\nsentences:\nif\n(sentence\nin\nsentence_value)\nand\n(sentence_value[sentence]\n>\n(\n1.2\n*\nthreshold)):\nsummary\n+=\n\" \"\n+\nsentence\nreturn\nsummary\nAbstractive Summarization\nIn abstractive summarization, the model forms its own phrases and sentences to provide a consistent summary. Abstractive summarization does not simply copy the sentences to form the summary but create new phrases that are relevant to the original document. This summarization technique uses deep learning techniques (like seq2seq) to paraphrase and shorten the original document.\nAbstractive Summarization using Transformers\nTransformers is an architecture which uses attention mechanisms to solve sequence to sequence problems while solving long term dependencies. Ever since it was introduced in 2017, transformers have been widely used in various NLP tasks such as text generation, question answering, text classification, language translation and so on. The transformer architecture consists of encoder and decoder parts. The encoder component consists of 6 encoders each of which consists of two sub layers: self-attention and feed forward networks. The input text is first converted into vectors using text embedding methods. Then the vector is passed into the self attention layer and the output from the self attention layer is passed through the feed forward network. The decoder also consists of both self attention and feed forward network layer. An additional layer is present in between these components which is an attention layer that helps the decoder to focus on the relevant parts of the input sentence.\nFig. Transformer architecture (from original paper)\nHuggingface Transformers provide various pre-trained models to perform NLP tasks. It provides APIs and tools to download and train state-of-the-art pre-trained models. Not only NLP, huggingface supports Computer Vision tasks like image classification, object detection and segmentation, audio classification and recognition, and multimodal tasks like table question answering, optical character recognition, and many more.\nBasic transformer pipeline for summarization\nHuggingface transformers provide an easy to use model for inference using pipeline. These pipelines are the objects that hide complex code and provide a simple API to perform various tasks.\nfrom\ntransformers\nimport\npipeline\nclassifier\n=\npipeline(\n\"summarization\"\n)\ntext\n=\n\"\"\"Acnesol Gel is an antibiotic that fights bacteria. It is used to treat acne, which appears as spots or pimples on your face, chest or back. This medicine works by attacking the bacteria that cause these pimples.Acnesol Gel is only meant for external use and should be used as advised by your doctor. You should normally wash and dry the affected area before applying a thin layer of the medicine. It should not be applied to broken or damaged skin.  Avoid any contact with your eyes, nose, or mouth. Rinse it off with water if you accidentally get it in these areas. It may take several weeks for your symptoms to improve, but you should keep using this medicine regularly. Do not stop using it as soon as your acne starts to get better. Ask your doctor when you should stop treatment. Common side effects like minor itching, burning, or redness of the skin and oily skin may be seen in some people. These are usually temporary and resolve on their own. Consult your doctor if they bother you or do not go away.It is a safe medicine, but you should inform your doctor if you have any problems with your bowels (intestines). Also, inform the doctor if you have ever had bloody diarrhea caused by taking antibiotics or if you are using any other medicines to treat skin conditions. Consult your doctor about using this medicine if you are pregnant or breastfeeding.\"\"\"\nclassifier(text)\nResult:\n[{\n'summary_text'\n:\n' Acnesol Gel is an antibiotic that fights bacteria that causes pimples . It is used to treat acne, which appears as spots or pimples on your face, chest or back . The medicine is only meant for external use and should be used as advised by your doctor .'\n}]\nThe\npipeline()\ntakes the name of the task to be performed (if we want to perform a question-answering task, then we can simply pass \u201cquestion-answering\u201d into the pipeline() and it automatically loads the model to perform the specific task.\nFine-tuning summarization model for medical dataset\nSummarization using abstractive technique is hard as compared to extractive summarization as we need to generate new text as the output. Different architectures like GTP, T5, BART are used to perform summarization tasks. We will be using the PubMed dataset. It contains datasets of long and structured documents obtained from PubMed OpenAccess repositories. from datasets import load_dataset\npubmed\n=\nload_dataset(\n\"ccdv/pubmed-summarization\"\n)\nThe PubMed dataset contains article, abstract and section_names as columns. The first step after loading the dataset is tokenizing the training data. Tokenization is the process of splitting paragraphs, sentences into smaller units called tokens. tokenizer = AutoTokenizer.from_pretrained(\u2018facebook/bart-large-cnn\u2019)\nThe next step is to preprocess the data. Before training the data, we need to convert our data into expected model input format.\ndef\npreprocess_function(examples):\ninputs\n=\n[doc\nfor\ndoc\nin\nexamples[\n\"article\"\n]]\nmodel_inputs\n=\ntokenizer(inputs, max_length\n=\n1024\n, truncation\n=\nTrue\n)\nlabels\n=\ntokenizer(examples[\n\"abstract\"\n], max_length\n=\n128\n, truncation\n=\nTrue\n, padding\n=\nTrue\n)\nmodel_inputs[\n\"labels\"\n]\n=\nlabels[\n\"input_ids\"\n]\nreturn\nmodel_inputs\nWe need to apply the processing function over the entire dataset. Setting flag\nbatched=True\nhelps to speed up the processing of multiple elements of the dataset at once.\ntokenized_pubmed\n=\npubmed.\nmap\n(preprocess_function, batched\n=\nTrue\n)\nNext, we need to create a batch for all the examples. Huggingface provides a data collator to create a batch for the examples.\ntokenized_datasets\n=\ntokenized_pubmed.remove_columns(pubmed[\n\"train\"\n].column_names)\ndata_collator\n=\nDataCollatorForSeq2Seq(tokenizer\n=\ntokenizer, model\n=\n\"facebook/bart-large-cnn\"\n)\nHuggingface provides various pre-trained models that we can leverage to perform a variety of machine learning tasks.\nmodel\n=\nAutoModelForSeq2SeqLM.from_pretrained(model)\nBefore training the model, we need to define our training hyperparamaters using training arguments. Since text summarization is a sequence to sequence tasks, we are using Seq2SeqTrainingArguments. And, we need to define our trainer by passing training and test dataset along with training arguments.\n# training arguments\ntraining_arguments\n=\nSeq2SeqTrainingArguments(\noutput_dir\n=\n'./results'\n,\nevaluation_strategy\n=\n'epoch'\n,\nlearning_rate\n=\n2e-5\n,\nper_device_train_batch_size\n=\n8\n,\nper_device_eval_batch_size\n=\n8\n,\nweight_decay\n=\n0.01\n,\nsave_total_limit\n=\n3\n,\nnum_train_epochs\n=\n3\n,\n# remove_unused_columns=False,\n# fp16=True,\n)\ntrainer\n=\nSeq2SeqTrainer(\nmodel\n=\nmodel,\nargs\n=\ntraining_arguments,\ntrain_dataset\n=\ntokenized_pubmed[\n'train'\n],\neval_dataset\n=\ntokenized_pubmed[\n'validation'\n],\ntokenizer\n=\ntokenizer,\ndata_collator\n=\ndata_collator\n)\nThe last step is to call\ntrain()\nto fine-tune our model.\ntrainer.train()\nConclusion\nSummarization helps to generalize the long documents by paraphrasing the important sentences from the whole document. It is very helpful in various applications like summarizing legal contracts, medical documents, news information and many more."
      ]
    },
    {
      "title": "Autocorrect and Minimum Edit Distance \u2013 Prabin Nepal",
      "url": "https://nepalprabin.github.io/posts/2021-10-25-autocorrect-and-minimum-edit-distance.html",
      "relevance_score": -0.01664912700653076,
      "content": [
        "Autocorrect and Minimum Edit Distance \u2013 Prabin Nepal\nThis is my brief note from\nDeepLearning.AI\u2019s\nNLP Specialization Course\n.\nWhat is Autocorrect?\nAutocorrect is an application that changes misspelled word into a correct word. When writing messages or drafting an email, you may have noticied that if we type any words that is misspelled, then that word automatically gets corrected with correct spelling and based on the context.\nFig. Autocorrect in action in google document\nHow does autocorrect work??\nWhile typing the document we can see we get automatic correction in our document. The basic working of this automatic correction is:\nIdentifying a misspelled word\nFind the strings\nn\nedit distance away\nFilter the candidates\nCalculate word probabilities\nNow let\u2019s see each of the points in detail.\n1. Identifying a misspelled word:\nLet\u2019s say we are writing a sentence\nThis is a draft docment of the APIs\n. Here we can see clearly see that the word\ndocment\nis misspelled. But, how do we know that this is a misspelled word? Well, we will have a dictionary containing all correct words and if we do not encounter given string in the dictionary, that string is obviously a misspelled word.\nif\nword\nnot\nin\nvocab:\nmisspelled\n=\nTrue\n# If the word is not in vocab, we identify it as a misspelled word.\nWhile identifying a misspelled words, we are only looking at the vocab but not the context. Consider a sentence\nHello deah\n. Here,\ndear\nis misspelled as\ndeah\n. If we write\ndeer\ninstead of\ndear\n, then we would not be able to identify misspelled word because\ndeer\nis present in vocab, though it is contextually incorrect.\n2. Find strings n edit distance away\nEdit is an operation that is performed on a string to change it.\nTypes of edit: - Insert (add a letter)\n\u2018to\u2019: \u2018top\u2019, \u2018two\u2019\n- Delete (remove a letter)\n\u2018hat\u2019: \u2018ha\u2019, \u2018at\u2019, \u2018ht\u2019\n- Switch (swap 2 adjacent letters)\n\u2018eta\u2019: \u2018eat\u2019, \u2018tea\u2019\n- Replace (change 1 letter to another)\n\u2018jaw\u2019: \u2018jar\u2019, \u2018paw\u2019\nUsing these edits, we can find all possible strings that are\nn\nedits away.\n3. Filter candidates\nAfter findings strings that are n edit distance away, next step is to filter those strings. After applying edits, the strings are compared with the vocab, and if those strings are not present in vocab, they are discarded. This, way we get a list of actual words.\n4. Calculate the word probabilities\nThe final step is to calculate the word probabilities and find the most likely word from the vocab. Given the sentence\nI am learning AI because AI is the new electricity\n, we find occurrence of each word and calculate the probability. Probability of given word\nw\ncan be calculated as the ratio of the count of word\nw\nto the total size of the corpus. Mathematically:\n\\(P(w) = \\frac{C(w)}{V}\\)\n,\nwhere:\n\\(P(w) - Probability \\ of\\ a\\ word\\)\n\\(C(w) - Number \\ of \\ times \\ the \\ word \\ appears\\)\n\\(V    - Total \\ size \\ of \\ the \\ corpus\\)\nMinimum Edit Distance\nTill now, we have seen how edit distance works and what are its applications in NLP domain. Now let\u2019s look at the\nminimum edit distance\n.\nMinimum edit distance\nis the minimum number of edits required to transform one string to another. It is used in various applications such as spelling correction, machine translation, DNA sequencing, and many more. To calculate minimum edit distance, we use three types of operations which is also discussed above, i.e., insert, delete and replace.\nFor example: Consider source word\ndeer\nand target word\ndoor\n. To change source to target, we need to perform two replace operations i.e., replace each of\ne\nto\no\n. Here number of edits is 2, but in minimum distance distance, there are cost associated with different edit operations.\nEdit Operations\nCost\nInsert\n1\nDelete\n1\nReplace\n2\nUsing above table, the edit distance for the above problem is: Edit distance =\n2 x replace cost\n=\n2x2\n=\n4\nThis is a brute force method where we are simply looking at the source and target word to calculate the edit distance. If we have large sentence, calculating edit distance with mentioned approach becomes tedious. To make things simpler, we can opt for tabular method using Dynamic Programming approaches."
      ]
    },
    {
      "title": "Illustrated Vision Transformers \u2013 Prabin Nepal",
      "url": "https://nepalprabin.github.io/posts/2021-07-27-illustrated-vision-transformers.html",
      "relevance_score": 0.018735453486442566,
      "content": [
        "Illustrated Vision Transformers \u2013 Prabin Nepal\nIntroduction\nEver since Transformer was introduced in 2017, there has been a huge success in the field of Natural Language Processing (NLP). Almost all NLP tasks use Transformers and it\u2019s been a huge success. The main reason for the effectiveness of the Transformer was its ability to handle long-term dependencies compared to RNNs and LSTMs. After its success in NLP, there have been various approaches to its usage for Computer Vision tasks. This paper\nAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\nby Dosovitskiy et al.\u00a0proposes using the transformer and has achieved some great results in various Computer Vision tasks.\nVison Transformer (ViT) makes use of an extremely large dataset while training the model. While training on datasets such as ImageNet (paper labels ImageNet as a mid-sized dataset), the accuracies of the model fall below ResNets. This is because the Transformer lack inductive bias such as translation equivariance and locality, thus it does not generalize well when trained on insufficient data.\nOverview of Vision Transformer\nSplit image into patches\nProvide sequence of linear embeddings of these patches as an input to transformer (flattening the image) Here, image patches are treated as the same way as tokens (as in NLP tasks)\nAdd positional embeddings and a learnable embedding\nclass\n(similar to BERT) to each patch embeddings\nPass these (patch + positional +\nclass\n] embeddings through Transformer encoder and get the output values for each\nclass\ntokens\nPass the representation of\nclass\nthrough MLP head and get the final class predictions.\nMethod\nSource:\nGoogle AI Blog\nFigure above depicts the overview of Vision Transformer. As shown in the figure, given the image, the image is split into patches. These image patches are flattened and passed to transformer encoder a sequence of tokens. Along with patch embeddings, position embedding is also passed as an input to the transformer encoder. Here position embedding is added along with patch embedding to retain positional information.\nHow is an image changed into a sequence of vectors to feed into the transformer encoder?\nLet\u2019s decode above figure by taking a RGB image of size\n\\(256 * 256 * 3\\)\n. The first step is to create patches of size\n\\(16 * 16\\)\nfrom input image. We can create\n\\(16 * 16 = 256\\)\ntotal patches. After splitting input images into patches, another step is to lineary place all splitted images. As seen in the figure, first patch is placed on the left most side and right most on the far right. Then, we linearly project these patches to get\n\\(1 * 768\\)\nvector representations. These representation is known as patch embeddings. The size of patch embedding becomes\n\\(256 * 768\\)\n(since we have 256 total patches with each patch represented as\n\\(1 * 768\\)\nvector.\nNext, we prepend learnable embedding\nclass\ntoken and position embeddings along with patch embeddings making the size\n\\(257 * 768\\)\n. Here, position embeddings are used to retain positional information. After converting images into vector representation, we need to send image in order as transformer doesnot know the order of the patches unlike CNNs. Due to this, we need to manually add some information about the position of the patches.\nComponents of Vision Transformer\nSince Vision Transformer is based on standard transformer architecture, only difference it being used for image tasks rather than for text, components used here is almost the same. Here, we discuss the components used in Vision transformer along with its significance.\nSide note: If you want to dive deep into transformer, then\nhere\nby Jay Alammar is a good place to start with.\nPatch embeddings\nAs the name of the paper \u201cAn Image is worth\n\\(16 * 16\\)\nwords transformers\u201d, the main take away of the paper is the breakdown of images into patches. Given the image:\n\\(x \\, \\varepsilon \\, \\mathbb{R}^{(H * W * C)}\\)\nit is reshaped into 2D flattened patches\n\\(x_p \\, \\varepsilon \\, \\mathbb{R}^{N*(P^2.C))}\\)\n, where, N=\n\\(\\frac{H.W}{p^2}\\)\n,\n\\((P, P)\\)\nis the resolution of each image patch.\nLearnable embedding\nclass\nA learnable embeding is added to the embeded patches\n\\(z_0^0 = x_{class}\\)\n. The state of this embedding class at the output of Transformer encoder\n\\(z_L^0\\)\nserves as the representation\n\\(y\\)\n. This classification head is attached to\n\\(z_L^0\\)\nduring both pre-training and fine-tuning.\nPosition Embeddings\nPosition Embeddings are added to the patch embeddings along with\nclass\ntoken which are then fed into the transformer encoder.\nTransformer Encoder\nThe transformer encoder is a standard transformer encoder architecture as presented in original transformer\npaper\n. This encoder takes embedded patches (patch embedding, position embedding and\nclass\nembedding). The transformer encoder consists of alternating layers of multuheaded self-attention and MLP blocks. Layer Normalization is used before every block and residual connection is used after every block.\nUsing hybrid architecture\nPreviously, image patches were used to form input sequence, another approach to form input sequence can be the feature map of a CNN (Convolution Neural Network). Here, the patches extracted from CNN map is used as patch embedding. From the paper:\nAs an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN. In this hybrid model, the patch embedding projection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case, the patches can have spatial size 1x1, which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension.\nReferences\nAn image is worth 16 * 16 words: Transformers for image recognition at scale\nViT Blog - Aman Arora\nThe AI Summer"
      ]
    },
    {
      "title": "Paper Explanation: A Simple Framework for Contrastive Learning of Visual Representations (simCLR) \u2013 Prabin Nepal",
      "url": "https://nepalprabin.github.io/posts/2021-03-26-simclr-explained.html",
      "relevance_score": -0.0010458604665473104,
      "content": [
        "Paper Explanation: A Simple Framework for Contrastive Learning of Visual Representations (simCLR) \u2013 Prabin Nepal\nVarious self-supervised learning methods have been proposed in recent years for learning image representations. Though a lot of methods have been proposed, the performance of those methods was found less effective in terms of accuracy than those of supervised counterparts. But\nSimCLR\nhas provided promising results, thus taking self-supervised learning to a new level. It uses a contrastive learning approach. This paper introduces a simple framework to learn representations from unlabeled images based on heavy data augmentation. Before going deep into simCLR and its details, let\u2019s see what contrastive learning is:\nContrastive Learning\nContrastive learning is a framework that learns similarities/dissimilarities from data that are organized into similar/dissimilar parts. It can also be considered as learning by comparing. Contrastive learning learns by comparing among different samples. The samples can be performed between positive pairs of \u2018similar\u2019 inputs and negative pairs of \u2018dissimilar\u2019 inputs.\nTo illustrate this in another way, you\u2019re told to chose a picture that is similar to the picture on the left i.e, cat (on the image below). You look at the picture and find the image from a bunch of images present(on the right side) that is similar to the cat. This way, you contrast between similar and dissimilar objects. The same is the case with contrastive learning. Using this approach we can train a machine learning model to classify between similar and dissimilar objects.\nSource: GoogleAI\nContrastive learning approaches only need to define the similarity distribution in order to sample a positive input\n\\(x^{+} \\sim\\ p^{+}\\)\nContrastive learning approaches only need to define the similarity distribution in order to sample a positive input\n\\(x^{+} \\sim\\ {p^{+}(.|x)}\\)\n, and a data distribution for a negative input\n\\(x^{-} \\sim\\ p^{-}(.|x)\\)\n, with respect to an input sample\n\\(x\\)\n. The goal of Contrastive learning is: the representation of similar samples should be mapped close together, while that of dissimilar samples should be further away from embedding space.\nSource:\nContrastive Representation Learning: A Framework and Review\nA Simple Framework for Contrastive Learning of Visual Representations\u200a-\u200aSimCLR\nHow does simCLR learn representations?\nsimCLR learns representations by maximizing agreement between differently augmented views of the same data example via a contrastive loss.\nInorder to learn good contrastive representation learning, simCLR consists of four major components\n-\nData Augmentation module\n: Data augmentation is more beneficial for unsupervised contrastive learning than supervised learning. The data augmentation module transforms any given data example into two correlated views of the same example. These examples are denoted as\n\\(\\\\widetilde{x\\_i}\\)\nand\n\\(\\\\widetilde{x\\_j}\\)\n, considered as positive pair. The authors mainly applied three augmentations sequentially:\nrandom cropping\nfollowed by resizing to the original size, random\ncolor distortions\n, and random\nGaussian blur\n.\nEncoder\n: A neural base encoder\n\\(f(.)\\)\nis used that extracts features from augmented data examples. ResNet is used as the architecture to extract those representations. The learned representation is the result of the average pooling layer.\nProjection head\n: The projection head\n\\(g(.)\\)\nis a MLP with one hidden layer that maps representations from the base encoder network to space where contrastive loss is applied. Here ReLU activation function is used for non-linearity.\nContrastive loss function\n: For any given set of\n\\(\\\\widetilde{x\\_k}\\)\nwhich includes positive example pair\n\\(\\\\widetilde{x\\_i}\\)\nand\n\\(\\\\widetilde{x\\_j}\\)\n, contrastive prediction task aims to identify\n\\(\\\\widetilde{x\\_j}\\)\nin {\n\\(\\\\widetilde{x\\_k}\\)\n} (here i and k are not equal) for given\n\\(\\\\widetilde{x\\_i}\\)\nsimCLR Framework\nGoogle AI\nWorking of simCLR algorithm\nFirst, we generate a batch of N examples and define contrastive prediction tasks on augmented examples. After applying a series of data augmentation techniques random(crop + resize + color distortion + grayscale) on N examples, 2N data points are generated (since we are generating similar pairs in a batch). Each augmented image is passed in a pair through the base encoder to get a representation from the image. Followed by the base encoder, a projection head is used that maps the base encoder to the representation\n\\(z\\_i\\)\nand\n\\(z\\_j\\)\nas presented in the paper. For each augmented image, we get embedding vectors for it. These embedding vectors are later subjected for calculating loss.\nsimCLR algorithm\nCalculating loss\nAfter getting the representations of the augmented images, the similarity of those images is calculated using cosine similarity. For two augmented image,\n\\(x\\_i\\)\nand\n\\(x\\_j\\)\n, cosine similarity is calculated on projected representations\n\\(z\\_i\\)\nand\n\\(z\\_j\\)\n.\n\\(s\\_{i,j} = \\\\frac{z\\_i^{T}z\\_j}{||z\\_i||||z\\_j||}\\)\n, where\n\\(T\\)\ndenotes a temperature parameter,\n\\(||z||\\)\nis the norm of the vector\nsimCLR uses\nNT-Xent\n(Normalized temperature-scaled cross entropy loss) for calculating the loss.\nHere\n\\(z\\_i\\)\nand\n\\(z\\_j\\)\nare the output vectors obtained from the projection head\nAfter training simCLR on the contrastive learning task, it can be used for transfer learning. For downstream tasks, representations from encoder are used rather than the representation from projection head. These representations can be used for tasks such as classification, detection.\nResults\nThe proposed simCLR outperformed previous self-supervised and semi-supervised methods on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100\u00d7 fewer labels.\nImageNet Top-1 accuracy of linear classifiers trained\non representations learned with different self-supervised methods (pre-trained on ImageNet). Gray cross indicates supervised\nResNet-50. Our method, SimCLR, is shown in bold\nReferences\n\u201cA Simple Framework for Contrastive Learning of Visual Representations\u201d\n\u201cSimCLR Slides, Google Brain Team\u201d\nContrastive Representation Learning: A Framework and Review\nThe Illustrated SimCLR Framework"
      ]
    },
    {
      "title": "Deep Residual Learning for Image Recognition (ResNet paper explained) \u2013 Prabin Nepal",
      "url": "https://nepalprabin.github.io/posts/2021-01-01-deep-residual-learning-for-image-recognition-resnet-paper-explained.html",
      "relevance_score": 0.013764035888016224,
      "content": [
        "Deep Residual Learning for Image Recognition (ResNet paper explained) \u2013 Prabin Nepal\nDeep Neural Networks tend to provide more accuracy as the number of layers increases. But, as we go more deeper in the network, the accuracy of the network decreases instead of increasing. As more layers are stacked, there occurs a problem of\nvanishing gradients\n. The paper mention that vanishing gradient has been addressed by normalized initialization and intermediate normalization layers. With the increase in depth, the accuracy gets saturated and then degrades rapidly.\n*Vanishing gradient: Vanishing gradient is a situation where a deep multilayer feedforward network or RNN is unable to propagate useful gradient information from output end of the model to the layers near the input end of the model. In this case, the gradient becomes very small and prevents weights from changing its value. It causes network hard to train.\nTraining error\n(left)\nand test error\n(right)\non CIFAR-10\nwith 20-layer and 56-layer \u201cplain\u201d networks\nsource\nThe above figure shows that with the increase in depth of the network, training error increases thus increasing test error. Here, the training error on 20 layer network is less than that of 56 layer network. Thus, the network cannot generalize well for new data and becomes an inefficient model. This degradation indicates that increasing the model layer does not aid in the performance of the model and not all the system are easy to optimize.\nThe paper address the degradation problem by introducing a deep residual learning framework. The main innovation for ResNet is the residual module. Residual module is specifically an identity residual module, which is a block of two convolutional layers with same number of filters and a small filter size. The output of the second layer is added with the input to the first convolution layer.\nResidual learning: a building block.\nsource\nNetwork Architecture\nThe paper took baseline model of VGGNet as a plain network with mostly 3x3 filters with two design rules: a) for the same output feature map size, the layers have the same number of filters and b) if the feature map size is halved, the number of filters is doubled so as to preserve the time complexity per layer. The network is ended with a global average pooling layer and a 1000-way fully connected layer with a softmax.\nBased on the plain network, shortcut connections are added to transform plain version into residual version.\nLeft\n: VGG-19 model\nMiddle\n: Plain network with 34 parameter layers\nRight\n: residual network with 34 parameter layers\nsource\nImplementation\nImage was first resized with its shorter side sampled into 256 x 480\nData augmentation techniques was carried out\nBatch normalization was carried out after each convolution and before activation\nStochastic gradient descent was used for training the network with mini batch of 256.\nWeight decay of 0.0001 and momentum of 0.9 was used.\nExperiments\nResnet architecture was evaluated on ImageNet 2012 classification dataset consisting of 1000 classes. The model was trained on the 1.28 million training images and evaluated on the 50k validation images. Moreover, 100k images were used for testing the model accuracy.\nWhile performing experiments on plain networks, a 34-layer plain network showed a higher validation error than an 18-layer plain network. Training error for the 34-layer plain network was found to be higher than the 18-layer plain network. Here, a degradation problem occurred as we go deep into the network. The deep plain networks may have a low convergence rate that impacts the accuracy of the model (impacts in reducing the training error).\nDifferent from the plain network, a shortcut connection was added to each pair of 3x3 filters. With a same number of layers as in plain network, Resnet 34 performed better than Resnet 18 network. Resnet-34 showed less error and performs well in generalizing validation data. This resolves the problem of degradation as seen on a plain deep network. The comparison for both plain and residual network is shown below:\nTraining on ImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain networks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to their plain counterparts.\nsource\nReferences\nDeep Residual Learning for Image Recognition"
      ]
    },
    {
      "title": "Self-supervised Learning \u2013 Prabin Nepal",
      "url": "https://nepalprabin.github.io/posts/2020-12-08-self-supervised-learning.html",
      "relevance_score": 0.10866151750087738,
      "content": [
        "Self-supervised Learning \u2013 Prabin Nepal\nI have been exploring self-supervised learning and been through papers and blogs to understand it. Self-supervised learning is considered the next big thing in deep learning and why not! If there is a way to learn without providing labels, then this enables us to leverage a large amount of unlabeled data for our tasks. I am going to provide my understanding of self-supervised learning and will try to explain some papers about it.\nWe have been familiar with supervised learning wherein we provide features and labels to train a model and the model uses those labels to learn from the features. But labeling data is not an easy task as it requires more time and manpower. There is a large amount of data being generated daily and is unlabeled. The generated data may be in the form of text, images, audio, or videos. Those data can be used for different purposes. But there is a catch. These data do not contain labels and it difficult to work on these sorts of data. Here comes self-supervised learning to the rescue.\nSo what is self-supervised learning and why is it needed?\nSelf-supervised learning is a learning framework that does not use human-labeled datasets to learn a visual representation of the data also known as representation learning. We have been familiar with the task such as classification, detection, and segmentation where a model is trained in a supervised manner which is later used for unseen data. These tasks are normally trained for specific scenarios, for e.g, the ImageNet dataset contains 1000 categories and can only recognize those categories. For categories that are not included in the ImageNet dataset, new annotations need to be done which is an expensive task. Self-supervised makes learning easy as it requires only unlabeled data to formulate the learning task. For training models in a self-supervised manner with unlabeled data, one needs to frame a supervised learning task (also known as\n*pre-text task\n). These pre-text tasks can later be used for\n**downstream\ntasks such as image classification, object detection, and many more.\n*Pre-text task: These are the tasks that are used for pre-training\n**Downstream task: These are the task that utilizes pre-trained model or components that can be used to perform tasks such as image recognition, segmentation.\nA general pipeline of self-supervised learning (\nsource\n)\nSelf-supervised Techniques for Images\nMany ideas have been proposed for self-supervised learning on images. A more common methodology or workflow is to train a model in one or multiple pretext tasks with the use of unlabeled data and use that model to perform downstream tasks. Some of the proposed ideas of self-supervised techniques for images are summarized below:\nRotation\nTo learn representation by predicting image rotations,\nGidaris et al.\nproposed an architecture where features are learned by training Convolution Nets to recognize rotations that are applied to the image before feeding to the network. This set of geometric transformations defines the classification pretext task that the model has to learn which can later be used for downstream tasks. Geometric transformation is made such that the image is rotated through 4 different angles (0, 90, 270, and 360). This way, our model has to predict one of the 4 transformations that are done on the image. To predict the task, our model has to understand the concept of objects such as their location, their type, and their pose.\nIllustration of the self-supervised task proposed for semantic feature learning.\nGiven four possible geometric transformations, the 0, 90, 180, and 270 degrees rotations,\na ConvNet model was trained to recognize the rotation that is applied to the image that it gets as input. (\nsource\n)\nMore details at:\nUnsupervised Representation Learning By Predicting Image Rotations\nExemplar\nIn Exemplar-CNN (\nDosovitskiy et al., 2015\n), a network is trained to discriminate between a set of surrogate classes. Each surrogate class is formed by applying random data augmentations such as translation, scaling, rotation, contrast and color shifts. While creating surrogate training data:\nN patches of size 32 x 32 pixels are randomly sampled from different images at varying positions. Since, we are interested in patches objects or parts of objects, random patches are sampled only from region containing considerable gradients.\nEach patch is applied with a variety of image transformations. All the resulting transformed patches are considered to be in same surrogate classes.\nThe pretext task is to discriminate between the set of surrogate class.\nSeveral random transformations applied to one of the\npatches extracted from the STL unlabeled dataset. The original\npatch is in the top left corner (\nsource\n)\nMore details at:\nDiscriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks\nJigsaw Puzzle\nAnother approach of learning visual representation from unlabeled dataset is by training a ConvNet model to solve Jigsaw puzzle as a pretext task which can be later used for downstream tasks. In Jigsaw puzzle task, model is trained to place 9 shuffled patches back to the original position. To place shuffled patches to original position,\nNoroozi et al.\nproposed a Context Free Network (CFN) which is a siamese CNN that uses shared weights. The patches are combined in a fully connected layer.\nLearning image representations by solving Jigsaw puzzles.\nThe image from which the tiles (marked with green lines) are extracted.\nA puzzle obtained by shuffling the tiles.\nDetermining the relative position between the central tile and the top two tiles from the left can be very challenging\nsource\nFrom the set of defined puzzle permutations, one permutation is randomly picked to arrange those 9 patches as per that permutation. This results CFN to return a vector with a probability value for each index. Given those 9 tiles, there will be 9! = 362,880 possible permutations.\u00a0This creates difficulty in jigsaw puzzles. To control this, the paper proposed to shuffle patches according to a predefined set of permutations and configured the model to predict a probability vector over all the indices in the set.\nContext Free Network Architecture (\nsource)\nMore details at:\nUnsupervised Learning of Visual Representations by Solving Jigsaw Puzzles\nRelative Patch Location\nThis approach by\nDoersch et al\n, predicts position of second patch of the image that is relative to the first patch. For this pretext task, a network is fed with two input patches and is passed through several convolutional layers. The network produces an output with probability to each of eight image patches. This can be taken as a classification problem with 8 classes where the input patch is assigned to one of these 8 classes to be considered as relative patch to the input patch.\nThe algorithm receives two patches in one of these eight\npossible spatial arrangements, without any context, and must then\nclassify which configuration was sampled (\nsource\n)\nMore details at:\nUnsupervised Visual Representation Learning by Context Prediction\nReferences:\nThe Illustrated Self-Supervised Learning\nSelf-supervised Learning\nSelf-supervised Visual Feature Learning with Deep Neural Networks: A Survey"
      ]
    },
    {
      "title": "MobileNet Architecture Explained \u2013 Prabin Nepal",
      "url": "https://nepalprabin.github.io/posts/2020-09-21-mobilenet-architecture-explained.html",
      "relevance_score": -0.003906812518835068,
      "content": [
        "MobileNet Architecture Explained \u2013 Prabin Nepal\nIn this blog post, I will try to write about the MobileNets and its architecture. MobileNet uses depthwise separable convolutions instead of standard convolution to reduce model size and computation. Hence, it can be used to build light weight deep neural networks for mobile and embedded vision applications.\nTopics Covered\nStandard convolutions and depthwise separable convolutions\nMobileNet Architecture\nWidth Multiplier to achieve thinner models\nResolution Multiplier for reduced representation\nArchitecture Implementation\nStandard convolutions and depthwise separable convolutions\nConvolution operation consists of an input image, a kernel or filter that slides through the input image and outputs a feature map. The main aim of convolution operation is to extract features from the input image. As we know, every image can be considered as a matrix of pixel values. Consider an input as 5x5 matrix with values of pixels 0 and 1 as shown below:\nAlso, consider another 3x3 matrix as below:\nThe convolution operation for input size 5x5 with filter of 3x3 is shown below:\nFig:\u00a0The Convolution operation.\nsource\nWe are sliding the 3x3 matrix over 5x5 input matrix and performing element-wise matrix multiplication and adding the multiplication output to get convolved feature. The output obtained from such operation is also called as feature map. The 3x3 matrix that is sliding over the input matrix is known as filter or kernel. More on convolution can be found at this amazing\narticle\n.\nSeparable Convolutions\nBefore knowing what depth-wise separable convolutions do, let\u2019s know about separable convolutions. There are two types of separable convolutions:\nspatial separable convolutions\nand\ndepthwise separable convolutions\n.\nSpatial separable convolutions\nSpatial Separable convolutions deals with spatial dimension of the image (width and height). It divides a kernel into two smaller kernel. For example, a\n\\(3\\*3\\)\nkernel is divided into a\n\\(3\\*1\\)\nand a\n\\(1\\*3\\)\nkernel.\nHere, instead of doing one convolution with 9 multiplicants, we can do two convolutions with 3 multiplications each (i.e., 6 in total) to achieve the same effect. With less multiplications, computational complexity goes down and network is able is run faster.\nOne of the famous convolution used to detect edges i.e., Sobel kernel can also be separated spatially.\nThough, less computation power is achieved using spatial separable convolution, all the kernels cannot be separated into two smaller kernels, which is one of the cons of spatial separable convolution.\nDepthwise Separable Convolutions\nDepthwise Separable Convolutions is what Mobilenet architecture is based on. Depthwise separable convolution works with kernel that cannot be factored into two smaller kernels. Spatial separable convolutions deals with spatial dimensions but depthwise separable convolutions deals with depth dimension also.\nDepthwise separable convolution is a factorized convolution that factorizes standard convolution into a depthwise convolution and a\n\\(1*1\\)\nconvolution called pointwise convolution. Depthwise separable convolutions splits kernel into two separate kernels for filtering and combining. Depthwise convolution is used for filterning whereas pointwise convolution is used for combining.\nUsing depthwise separable convolutions, the total computation required for the operation is the sum of depthwise convolution and pointwise convolution which is:\nFor standard convolution, total computation is:\n\\(D_K . D_K . M . N . D_F . D_F\\)\n, where computational cost depends on number of input channels\n\\(M\\)\n, number of output channels\n\\(N\\)\n, kernel size\n\\(D_K\\)\nand feature map size\n\\(D_F\\)\n.\nBy expressing convolution as a two steps process of filtering and combining, total reduction in computation is:\n\\(\\frac{D_K . D_K . M . D_F . D_F + M . N . D_F . D_F}{D_K . D_K . M. N. D_F . D_F}\\)\n, which is equivalent to\n\\(\\frac{1}{N} + \\frac{1}{D_k^2}\\)\nThat means when\n\\(D_K * D_K\\)\nis 3*3, computation cost can be reduced to 8 to 9 times.\nMore on depthwise separable convolutions can be found\nhere\n.\nMobileNet Architecture\nAs mentioned above, mobilenet is built on depthwise separable convolutions, except for first layer. First layer is a full convolutional layer.\nAll layers are followed by batch normalization and ReLU non-linearity. However, final layer is a fully connected layer without any non-linearity and feeds to the softmax for classification.\nFor down sampling, strided convolution is used for both depthwise convolution as well as for first fully convolutional layer.\nThe total number of layers for mobilenet is 28 considering depthwise and pointwise convolution as separate layers.\nfig.\u00a0(left) Standard convolution with batchnorm and relu\n(right) depthwise and pointwise convolution followed by batchnorm and relu\nsource\nMobileNet architecture is shown below:\nFig. MobileNet architecture\nWidth Multiplier to achieve Thinner Models\nThough our mobilenet model is smaller and computationally less expensive, sometimes we need our model to be more smaller and less expensive in terms of computation. To construct these models, a separate parameter\n\\(\\alpha\\)\nis used called as width multiplier. Width multiplier helps to make network thinner uniformly at each layer. For any given layer and width multiplier\n\\(\\alpha\\)\n, the number of input channels\n\\(M\\)\nbecomes\n\\(\\alpha M\\)\nand the number of output channel\n\\(N\\)\nbecomes\n\\(\\alpha N\\)\n. Then, the computational cost for depthwise separable convolution with width multiplier becomes:\n\\(D_K . D_K . \\alpha M . D_F . D_F + \\alpha M . \\alpha N . D_F . D_F\\)\nWidth multiplier can be applied to any model structure to define a new smaller model with a reasonable accuracy, latency and size trade off. It is used to define a new reduced structure that needs to be trained from scratch.\nMobileNet paper\nResolution Multiplier for reduced representation\nResolution Multiplier is another parameter for reducing model computational cost. It is represented by\n\\(\\rho\\)\n. It is applied to the input image and internal representation of every layer is reduced by the same multiplier. The computational cost for depthwise separable convolution with width multiplier becomes:\n\\(D_K . D_K . \\alpha M . \\rho D\\_F . \\rho D_F + \\alpha M . \\alpha N . \\rho D_F . \\rho D_F\\)\nThe value of\n\\(\\rho\\)\n= 1 is the base mobilenet and\n\\(\\rho<1\\)\nis the reduced computational MobileNets.\nArchitecture Implementation\nMobileNet uses depthwise separable convolutions where each layers is followed by BatchNormalization and ReLU non-linearity. MobileNet contains a depthwise and a pointwise convolution layer. The code snippets inspired from\nMLT\n.\n# First we will build mobilenet block\ndef\nmobilenet_block(x, filters, strides):\nx\n=\nkeras.layers.DepthwiseConv2D(kernel_size\n=\n3\n, strides\n=\nstrides, padding\n=\n'same'\n)(x)\nx\n=\nkeras.layers.BatchNormalization()(x)\nx\n=\nkeras.layers.ReLU()(x)\nx\n=\nkeras.layers.Conv2D(filters\n=\nfilters, kernel_size\n=\n1\n, strides\n=\n1\n, padding\n=\n'same'\n)(x)\nx\n=\nkeras.layers.BatchNormalization()(x)\nx\n=\nkeras.layers.ReLU()(x)\nreturn\nx\nMobileNet uses input_shape of 224*224*3. First layer of mobilenet is a Convolutional layer with 32 filters, 3*3 kernel and stride of 2. This is followed by BatchNormalization and ReLU non-linearity.\nINPUT_SHAPE\n=\n28\n,\n28\n,\n3\ninput\n=\nkeras.layers.Input(INPUT_SHAPE)\nx\n=\nkeras.layers.Conv2D(filters\n=\n32\n, kernel_size\n=\n3\n, strides\n=\n2\n, padding\n=\n'same'\n)(\ninput\n)\nx\n=\nkeras.layers.BatchNormalization()(x)\nx\n=\nkeras.layers.ReLU()(x)\nAfter first layers, there is a series of mobilenet block with different kernel sizes and filters.\nx\n=\nmobilenet_block(x, filters\n=\n64\n, strides\n=\n1\n)\nx\n=\nmobilenet_block(x, filters\n=\n128\n, strides\n=\n2\n)\nx\n=\nmobilenet_block(x, filters\n=\n128\n, strides\n=\n1\n)\nx\n=\nmobilenet_block(x, filters\n=\n256\n, strides\n=\n2\n)\nx\n=\nmobilenet_block(x, filters\n=\n256\n, strides\n=\n1\n)\nx\n=\nmobilenet_block(x, filters\n=\n512\n, strides\n=\n2\n)\nfor\n_\nin\nrange\n(\n5\n):\nx\n=\nmobilenet_block(x, filters\n=\n512\n, strides\n=\n1\n)\nx\n=\nkeras.layers.AveragePooling2D(pool_size\n=\n7\n, strides\n=\n1\n)(x)\noutput\n=\nkeras.layers.Dense(\n1000\n, activation\n=\n'softmax'\n)(x)\nReferences\nMobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\nMachine Learning Tokyo\nA Basic Introduction to Separable Convolutions\nAn Intuitive Explanation of Convolutional Neural\u00a0Networks"
      ]
    },
    {
      "title": "Neural style transfer and its working \u2013 Prabin Nepal",
      "url": "https://nepalprabin.github.io/posts/2020-08-23-neural-style-transfer-and-its-working.html",
      "relevance_score": -0.0008588922210037708,
      "content": [
        "Neural style transfer and its working \u2013 Prabin Nepal\nHave you ever used an app called Prisma that styles your image using popular paintings and turns your photo stunning? If that\u2019s the case then, the app you are using is the result of style transfer; a computer vision technique that combines your images with artistic style.\nIntroduction\nStyle transfer is a computer vision technique that takes two images: content image and style image, combines them to form a resulting image that style the image based on style image taking contents from the content image.\nHere is how it looks like:\nThe content image you can see above is Van Gogh\u2019s Starry Night Painting and the style image a image from Tubingen university from Germany. Resultant image is shown on the right side that used content of content image and is styled using style image.\nNow let\u2019s get into the working of neural style transfer\nNeural Style Transfer is based on Deep Neural Network that create images of high perpetual quality. It uses neural network to separate and recombine content and style of images that we feed to obtain the desired result. The original paper uses 19 layer VGG network comprising of 16 convolutional layers, 5 max-pooling layers and 3 fully connected layers.\nFig. A 19 layer VGG network\nsource\nHow exactly do we obtain such images?\nOur goal here is to apply style over our content image. We are not training any neural network in this case, rather we start from a blank image and optimize the cost function by changing the pixel values of the image. The cost function contains two losses: Content loss and Style loss.\nConsidering c as the content image and x as the style transferred image, content loss tends to\n\\(0\\)\nwhen\n\\(x\\)\nand\n\\(c\\)\nare close to each other and increases when these value gets increased.\nGiven the original image\n\\(vec{p}\\)\nand generated image\n\\(vec{x}\\)\n, we can define the loss generated by the content image as:\nContent loss takes content weight which is a scalar that gives weighting for the content loss, content_current that gives features of the current image. content_current is the Pytorch tensor having shape (1,\n\\(C_l\\)\n,\n\\(H_l\\)\n,\n\\(W_l\\)\n), where\n\\(C_l\\)\nis the number of channels in layer\n\\(l\\)\n,\n\\(H_l\\)\nand\n\\(W_l\\)\nare width and height.\nContent loss in python\ndef\ncontent_loss(content_weight, content_current, content_original):\nreturn\ntorch.\nsum\n(content_weight\n*\n(content_current\n-\ncontent_original)\n**\n2\n)\nAfter computing content loss, we can compute style loss.\nTo compute style loss, we need to first compute Gram matrix\n\\(G\\)\n. Gram matrix represents the correlation between responses of each filter. Given a feature map\n\\(F^l\\)\nof shape\n\\((C_l, M_l)\\)\n, the Gram matrix is given by:\n\\[G_{ij}^\\ell  = \\sum_k F^{\\ell}_{ik} F^{\\ell}_{jk}\\]\nGram matrix in python:\ndef\ngram_matrix(features, normalize\n=\nTrue\n):\nN, C, H, W\n=\nfeatures.size()\nfeatures\n=\nfeatures.reshape(N, C,\n-\n1\n)\ngram_matrix\n=\ntorch.zeros([N,C,C]).to(features.device).to(features.dtype)\nfor\ni\nin\nrange\n(N):\ngram_matrix[i,:]\n=\ntorch.mm( features[i,:], features[i,:].t())\nif\n(normalize):\ngram_matrix\n/=\nfloat\n(H\n*\nW\n*\nC)\nreturn\ngram_matrix\nNow implementing style loss:\ndef\nstyle_loss(feats, style_layers, style_targets, style_weights):\nloss\n=\n0\nfor\ni, layer\nin\nenumerate\n(style_layers):\ngram_feat\n=\ngram_matrix(feats[layer])\nloss\n+=\n(style_weights[i]\n*\ntorch.\nsum\n((gram_feat\n-\nstyle_targets[i])\n**\n2\n))\nreturn\nloss\nTo increase the smoothness in the image, we can use another term to our loss that penalizes total variation in the pixel values. We can compute the \u201ctotal variation\u201d as the sum of the squares of differences in the pixel values for all pairs of pixels that are next to each other (horizontally or vertically). Here we sum the total-variation regualarization for each of the 3 input channels (RGB), and weight the total summed loss by the total variation weight,\u00a0w_t\nTotal variational loss in python\ndef\ntv_loss(img, tv_weight):\nloss\n=\n0\nloss\n+=\ntorch.\nsum\n((img[:,:,\n1\n:,:]\n-\nimg[:,:,:\n-\n1\n,:])\n**\n2\n)\nloss\n+=\ntorch.\nsum\n((img[:,:,:,\n1\n:]\n-\nimg[:,:,:,:\n-\n1\n])\n**\n2\n)\nloss\n*=\ntv_weight\nreturn\nlos\nCombining above snippets together, we can generate resultant image using content and style images. The complete code is available on\ngithub\n. Code is the homework solution for\nDeep Learning for Computer Vision\ntaught by Justin Johnson.\nReferences\nImage Style Transfer Using Convolutional Neural Networks\nA Neural Algorithm of Artistic Style\nDeep Learning for Computer Vision"
      ]
    },
    {
      "title": "Deep Convolutional Generative Adversarial Networks (DCGANs) \u2013 Prabin Nepal",
      "url": "https://nepalprabin.github.io/posts/2020-08-15-deep-convolutional-general-adversarial-networks-dcgans.html",
      "relevance_score": 0.023659519851207733,
      "content": [
        "Deep Convolutional Generative Adversarial Networks (DCGANs) \u2013 Prabin Nepal\nDCGAN (Deep Convolutional General Adversarial Networks) uses convolutional layers in its design.\nArchitectural Details for DCGAN\nComprised convolutional network without max-pooling. Instead, it uses convolutional stride and transpose convolution for downsampling and upsampling respectively. To find out how pooling and convolutional stride differs please go through\nthis\n.\nRemoved all fully connected layers\nUsed batch normalization to bring stability in learning. It is done by normalizing the input to have zero mean and a variance of one. Batchnormalization was added to all the layers except generator output layer and discriminator input layer\nReLU activation is used in the generator except for the output layer which uses tanh activation function\nLeakyReLU activation is used at all layers in the discriminator\nTraining Generator in DCGAN\n[latexpage]\nGenerator takes a uniform noise distribution\n\\(z\\)\nas input. This input is reshaped with the help of fully connected layer into three dimensional layer with small base (width * height) and depth. Then, using transposed convolution, the output from previous layer is upsampled. Each transoposed convolution layer is followed by batch normalization to normalize the input. This helps in stabilizing the training of our GAN.\nsource\nDetails of Adversarial Training\nDCGAN was trained on three datasets: Large-scale Scene Understanding (LSUN), ImageNet-1k and Faces dataset.\nTraining images were scaled to the range of tanh activation function [-1, 1] and no further pre-processing was performed\nAll models were trained with mini-batch size of 128\nWeights were initialized from normal normal distribution with mean 0 and standard deviation of 0.2\nIncase of LeakyReLU, the value of alpha was set to 0.2\nAdam optimizer was used for updating weights. Learning rate was set to 0.001 and momentum term was reduced to 0.5 from 0.9\nDataset Details\nDCGAN model was trained on LSUN bedroom dataset comprising over 3 million training images.\nNo data augmentation was used\nDe-duplication process was performed to decrease the likelihood of generator memorizing input examples. For this, autoencoder was trained to find and delete similar points from the training dataset. De-duplication process helped in removing 275k images.\nGenerated bedrooms after five epochs of training\nsource\nGenerated bedrooms after five epochs of training\nsource\nReferences\nUnsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\nHenry AI Labs"
      ]
    },
    {
      "title": "General Adversarial Networks (GANs) \u2013 Prabin Nepal",
      "url": "https://nepalprabin.github.io/posts/2020-08-04-general-adversarial-networks-gans.html",
      "relevance_score": -0.06519220769405365,
      "content": [
        "General Adversarial Networks (GANs) \u2013 Prabin Nepal\n\u201c\nGeneral Adversarial Nets\nis the most interesting idea in the last 10 years in machine learning\u201d. This was the statement from Yann LeCun regarding GANs when Ian Goodfellow and co-authors introduced it in 2014. After its first introduction, many research papers are published with various architectures and its use cases.\nSo what are General Adversarial Networks? What are its use cases. In this post I will try to explain about GANs, its underlying math, use cases and GAN implementation in keras.\nIntroduction\nAs stated by Ian Goodfellow on his\npaper\n, GAN is a framework for estimating generative models via an adversarial process. During this process, two models are trained. One is called generator\n\\(G\\)\nand another model is called as discriminator\n\\(D\\)\n. Generator\n\\(G\\)\ngenerates new examples that are similar to original data. Discriminator model\n\\(D\\)\nclassifies whether the data is real or fake. To keep in simple terms, generator is analogous to counterfeiters, whereas discriminator is analogous to police. Counterfeiters tries to produce fake currency and use it, while police try to detect the fake currency. Counterfeiters come up with new ideas and patterns to make the fake money as similar to the original and fool the police. Similarly, police tries to detect the fake money. Similar is the case with GAN. Generative model tries to create fake data samples and fool the discriminator and discriminator classifies whether data is fake or not. This process goes on until data samples generated by generator are indistinguishable from discriminator.\nConsider the following notations for different data points and distributions:\nGenerator\u2019s distribution:\n\\(p_g\\)\nData:\n\\(x\\)\nInput noise variables:\n\\(p_z(z)\\)\nThen,\n\\(G\\)\nis a generator model represented by multilayer perceptron with parameters\n\\(\\theta_g\\)\n(parameters of weights and biases).\nSimilarly,\n\\(D\\)\nis a discriminator model also represented by multilayer perceptron\n\\(D(x;\\theta\\_d)\\)\n. Then,\n\\(D(x)\\)\nrepresents a probability that data\n\\(x\\)\ncame from original distribution rather than\n\\(p_g\\)\n.\nA known dataset serves as input for the discriminator. Training involves presenting samples from the training dataset until it achieves acceptable accuracy. Generator however trains based on whether it fools the discriminator. The input to generator is the data samples from latent space ( e.g, multivariate normal distribution). Then, the output generated by the generator is evaluated by the discriminator. Both generator and discriminator model goes through backpropagation to reduce the loss. During this step, generator generates better data samples (say images), whereas discriminator becomes good in classifying fake samples coming from the generator. This way, discriminator\n\\(D\\)\nand generator\n\\(G\\)\nplay two-player min-max game.\nTraining procedure for GANs\nTake a random noise vector\n\\(z\\)\nand feed to the generator\n\\(G\\)\nto produce fake examples\n\\(x^*\\)\n. Here label y=0 for\n\\((x, y)\\)\ninput-output pair.\nTake fake data\n\\(x^*\\)\nand real data\n\\(x\\)\nand feed to the discriminator model alternatively.\nSince, discriminator\n\\(D\\)\nis a multilayer perceptron, it outputs value between 0 and 1. These values indicates the probability that input is real.\nBoth generator and discriminator calculates their respective loss and perform backpropagation to reduce the loss.\nDiscriminator tries to maximize the probability of assigning correct labels to both original data and data from random samples.\nSimilarly, generator tries to minimize the discriminator\u2019s ability to detect correct and fake samples.\nThese two networks go on competing with each other until they reach Nash equilibrium. Nash equilibrium is a point in a game where neither player can improve their situation by changing their strategy. More on Nash equilibrium can be found\nhere\n. The overview of GAN architecture is shown below:\nFig. GAN\nThe noise vector z is transformed into x* by a generator model which is then fed into discriminator network. Similarly, data from original sample is also fed into discriminator. The discriminator in result outputs a classification values close to 1 for real data x. While for data x*, discriminator tries to output value 0 indicating that x* is fake.\nDerivation of Loss function for GANs\nSince, GAN is trained in multilayer perception, its loss can be calculated using cross-entropy loss given as:\n\\(L(y, y\\hat{})\\)\n=\n\\([y\\log y\\hat{} + (1-y) log(1-y\\hat{} )]\\)\nThe label for the data coming from\n\\(p_{data}(x)\\)\nis\n\\(y = 1\\)\nand\n\\(y\\hat{}\\)\n=\n\\(D(x)\\)\n.\nSo, the cross-entropy equation becomes:\n\\(L(D(x), 1)\\)\n=\n\\(log(D(x))\\)\n\u2014\u2014\u2014\u2014\u2013(A)\nSimilarly, for data coming from generator the label is\n\\(y=0\\)\nand\n\\(y\\hat{}=D(G(z))\\)\nIn this case, our cross entropy equation becomes:\n\\(L(D(G(z)), 0)\\)\n=\n\\((1-0) log(1-D(G(z))\\)\n=\n\\(log(1-D(G(z))\\)\n\u2014\u2014\u2014\u2014\u2013(B)\nWe know that the objective of discriminator is to correctly classify fake versus real data. To achieve this equations (A) and (B) should be maximized.\n\\(max\\)\n{\n\\(log (D(x))\\)\n+\n\\(log(1-D(G(z)))\\)\n}\nThe role of generator is to fool discriminator so as to predict fake data as real, i.e., to achieve\n\\(D(G(z)) = 1\\)\n. So, the objective function for generator is given as:\n\\(min\\)\n{\n\\(log (D(x))\\)\n+\n\\(log(1-D(G(z)))\\)\n}\nNote:\n\\(log(D(x))\\)\nhas nothing to do with generator objective function. It is kept to provide compact representation of generator and discriminator objective function in our equation.\nCombining the objective function of both discriminator and generator, we get following equation:\n\\(\\min\\_{G}\\)\n\\(\\max\\_{D}\\)\n{\n\\(log (D(x))\\)\n+\n\\(log(1-D(G(z)))\\)\n}\nAll the above equations are written with respect to a single instance of data point (x). To consider all the instances of x, we need to take expectation of the whole arguments present in the equation which results in the following equation:\nApplications of GANs\nImage-to-Image Translation\nImage-to-Text Translation\nTo generate realistic photographs\nPhoto Inpainting and many more\nImplementation of GAN\nSince, GANs consists of two models, generator and discriminator model, we need to build two models. Before building models let\u2019s import libraries. The code snippets for GAN implementation is taken from\nGANs in Action\nbook.\nfrom\nkeras.datasets\nimport\nmnist\nfrom\nkeras.layers\nimport\nDense, Flatten, Reshape\nfrom\nkeras.layers.advanced_activations\nimport\nLeakyReLU\nfrom\nkeras.models\nimport\nSequential\nfrom\nkeras.optimizers\nimport\nAdam\nSince we will use mnist data to train our discriminator, we need 28*28 image size for our generator for generating new images.\nimg_rows\n=\n28\nimg_cols\n=\n28\nchannels\n=\n1\nimg_shape\n=\n(img_rows, img_cols, channels)\nz_dim\n=\n100\nz_dim is the size of noise vector used as input to generator model\nNow, we\u2019ll build a generator model\ndef\nbuild_generator(img_shape, z_dim):\nmodel\n=\nSequential()\nmodel.add(Dense(\n128\n, input_dim\n=\nz_dim))\nmodel.add(LeakyReLU(alpha\n=\n0.01\n))\nmodel.add(Dense(\n28\n*\n28\n*\n1\n, activation\n=\n'tanh'\n))\nmodel.add(Reshape(img_shape))\nreturn\nmodel\nSimilarly, building generator model\ndef\nbuild_discriminator(img_shape):\nmodel\n=\nSequential()\nmodel.add(Flatten(input_shape\n=\nimg_shape))\nmodel.add(Dense(\n128\n))\nmodel.add(LeakyReLU(alpha\n=\n0.01\n))\nmodel.add(Dense(\n1\n, activation\n=\n'sigmoid'\n))\nreturn\nmodel\nNow, we will build GAN using generator and discriminator build previously. While using combined model to train generator, we keep the parameters of discriminator model fixed. Also, discriminator is trained as an independently compiled model.\ndef\nbuild_gan(generator, discriminator):\nmodel\n=\nSequential()\nmodel.add(generator)\nmodel.add(discriminator)\nreturn\nmodel\ndiscriminator\n=\nbuild_discriminator(img_shape)\ndiscriminator.\ncompile\n(loss\n=\n'binary_crossentropy'\n, optimizer\n=\nAdam(), metrics\n=\n[\n'accuracy'\n])\ngenerator\n=\nbuild_generator(img_shape, z_dim)\ndiscriminator.trainable\n=\nFalse\ngan\n=\nbuild_gan(generator, discriminator)\ngan.\ncompile\n(loss\n=\n'binary_crossentropy'\n, optimizer\n=\nAdam())\nNow, that we have build our GAN model, we now train our GAN model. MNIST images is taken as real examples and fake image is generated from noise vector z. These are used to train discriminator network while keeping generator\u2019s parameters constant. Similarly, fake images are generated and we used those images to train generator network by keeping discriminator\u2019s parameter constant.\nThe images produced by generator over the course of training iterations is shown below\nDuring training, random noise is generated and generator gradually learns to imitate the features of training dataset.\nThis is the output from two layer general adversarial networks. It is gradually imitating the features of MNIST images.\nReferences\nIan J. Goodfellow et. al.\nGenerative Adversarial Nets\n.\nhttps://en.wikipedia.org/wiki/Generative_adversarial_network\nAhlad Kumar (\nGAN youtube playlist\n)"
      ]
    },
    {
      "title": "Paper Explanation: Going deeper with Convolutions (GoogLeNet) \u2013 Prabin Nepal",
      "url": "https://nepalprabin.github.io/posts/2020-06-05-paper-explanation-going-deeper-with-convolutions-googlenet.html",
      "relevance_score": 0.07450544834136963,
      "content": [
        "Paper Explanation: Going deeper with Convolutions (GoogLeNet) \u2013 Prabin Nepal\nGoogle proposed a deep Convolution Neural Network named inception that achieved top results for classification and detection in ILSVRC 2014.\nThe ImageNet Large Scale Visual Recognition Challenge (ILSVRC) evaluates algorithms for object detection and image classification at large scale. One high level motivation is to allow researchers to compare progress in detection across a wider variety of objects \u2013 taking advantage of the quite expensive labeling effort. Another motivation is to measure the progress of computer vision for large scale image indexing for retrieval and annotation\nhttp://www.image-net.org/challenges/LSVRC/\n\u201cGoing deeper with convolutions\u201d is actually inspired by an internet meme: \u2018We need to go deeper\u2019\nIn ILSVRC 2014, GoogLeNet used 12x fewer parameters than\nAlexNet\nused 2 years ago in 2012 competition.\nProblems Inception v1 is trying to solve\nThe important parts in the image can have large variation in size. For instance, image of object can be in various positions and some pictures are zoomed in and other may get zoomed out. Because of such variation in images, choosing the right kernel size for performing convolution operation becomes very difficult. We require a larger kernel to extract information of object that is distributed more in the picture and a smaller kernel is preferred to extract information of image that is distributed less in the picture.\nOne of the major approach to increase the performance of neural networks in by increasing its size. This includes increasing its depth and also its size. Bigger size of neural networks corresponds to larger number of parameters, that makes network more prone to overfitting, especially when labeled training examples are limited.\nAnother drawback of increased network size is increased use of computational resources. If more convolution layers are chained then there results in more consumption of computation resources. If these added capacity is used ineffectively, then computation resources get wasted.\nSolution\nTo solve these issues, this paper comes up with the solution to form a \u2018wider\u2019 network rather than \u2018deeper\u2019 which is called as Inception module.\nThe \u2018naive\u2019 inception module performs convolutions on input from previous layer, with 3 different size of kernels or filters specifically 1x1, 3x3, and 5x5. In addition to this, max pooling is also performed. Outputs are then concatenated and sent to the next inception module.\nOne problem to the \u2018naive\u2019 approach is, even having 5x5 convolutions can lead to require large resource in terms of computations. This problem emerges more once pooling is added.\nTo make our networks inexpensive computationally, authors applied dimensionality reductions by adding 1x1 convolutions before 3x3 and 5x5 convolutions. Let\u2019s see how these affect the number of parameters in the neural network.\nLet\u2019s see what 5x5 convolution would be computationally\nComputation for above convolution operation is:\n(5\u00b2)(192)(32)(28\u00b2) = 120,422,400\noperations\nTo bring down such a great number of operations, dimensionality reduction can be used. Here, it is done by convolving with 1x1 filters before performing convolution with bigger filters.\n5\u00d75 Convolution with Dimensionality Reduction\nAfter dimensionality reduction number of operations for 5x5 convolution becomes:\n(1\u00b2)(192)(16)(28\u00b2) = 2,408,448\noperations for the\n1 \u00d7 1\nconvolution and,\n(5\u00b2)(16)(32)(28\u00b2) = 10,035,200\noperations for the\n5 \u00d7 5\nconvolution.\nIn total there will be\n2,408,448 + 10,035,200 = 12,443,648\noperations. There is large amount of reduction in computation.\nSo, after applying dimensionality reduction, our inception module becomes:\nGoogLeNet was built using inception module with dimensionality reduction. GoogLeNet consists of 22 layers deep network (27 with pooling layers included). All the convolutions, including the convolutions inside inception module , uses rectified linear activation.\nGoogLeNet incarnation of the Inception architecture.\nSource: Original Paper\nAll the convolutions, including those inside the Inception modules, use rectified linear activation. The size of the receptive field in our network is 224x224 taking RGB color channels with mean sub-traction. \u201c#3x3reduce\u201d and \u201c#5x5reduce\u201d stands for the number of 1x1 filters in the reduction layer used before the 3x3 and 5x5 convolutions. One can see the number of 1x1 filters in the pro-jection layer after the built-in max-pooling in the pool proj column. All these reduction/projection layers use rectified linear activation as well\nOriginal Paper\nGoogLeNet is 22 layer deep counting only layers with parameters. With such deep network the may arise a problem such as vanishing gradient. To eliminate this, authors introduced auxiliary classifiers that are connected to intermediate layers, and helps the gradient signals to propagate back. These auxiliary classifiers are added on top of the output of Inception (4a) and (4d) modules. The loss from auxiliary classifiers are added during training and discarded during inference.\nThe exact structure of the extra network on the side, including the auxiliary classifier, is as follows:\nAn average pooling layer with 5x5 filter size and stride 3, resulting in an 4x4 512 output for the (4a), and 4x4 528 for the (4d) stage.\nA 1x1 convolution with 128 filters for dimension reduction and rectified linear activation.\nA fully connected layer with 1024 units and rectified linear activation.\nA dropout layer with 70% ratio of dropped outputs\nA linear layer with softmax loss as the classifier (predicting the same 1000 classes as the main classifier, but removed at inference time).\nThe systematic view of GoogLeNet architecture is shown below:\nGoogLeNet architecture\nGoogLeNet consists of a total of 9 inception modules namely 3a, 3b, 4a, 4b, 4c, 4d , 4e, 5a and 5b.\nGoogLeNet implementation\nHaving known about inception module and its inclusion in GoogLeNet architecture, we now implement GoogLeNet in\ntensorflow\n. This implementation of GoogLeNet is inspired from analytics vidya\narticle\non inception net.\nImporting the required libraries:\nfrom\ntensorflow.keras.layers\nimport\nLayer\nimport\ntensorflow.keras.backend\nas\nK\nimport\ntensorflow\nas\ntf\nfrom\ntensorflow.keras.datasets\nimport\ncifar10\nfrom\ntensorflow.keras.models\nimport\nModel\nfrom\ntensorflow.keras.layers\nimport\nConv2D, MaxPool2D, Dropout, Dense, Input, concatenate, GlobalAveragePooling2D, AveragePooling2D, Flatten\nimport\ncv2\nimport\nnumpy\nas\nnp\nfrom\nkeras.utils\nimport\nnp_utils\nimport\nmath\nfrom\ntensorflow.keras.optimizers\nimport\nSGD\nfrom\ntensorflow.keras.callbacks\nimport\nLearningRateScheduler\nNext we are using cifar10 dataset as our data.\nnum_classes\n=\n10\ndef\nload_cifar_data(img_rows, img_cols):\n#Loading training and validation datasets\n(X_train, Y_train), (X_valid, Y_valid)\n=\ncifar10.load_data()\n#Resizing images\nX_train\n=\nnp.array([cv2.resize(img, (img_rows, img_cols))\nfor\nimg\nin\nX_train[:,:,:,:]])\nX_valid\n=\nnp.array([cv2.resize(img, (img_rows, img_cols))\nfor\nimg\nin\nX_valid[:,:,:,:]])\n#Transform targets to keras compatible format\nY_train\n=\nnp_utils.to_categorical(Y_train, num_classes)\nY_valid\n=\nnp_utils.to_categorical(Y_valid, num_classes)\nX_train\n=\nX_train.astype(\n'float32'\n)\nX_valid\n=\nX_valid.astype(\n'float32'\n)\n#Preprocessing data\nX_train\n=\nX_train\n/\n255.0\nY_train\n=\nX_valid\n/\n255.0\nreturn\nX_train, Y_train, X_valid, Y_valid\nX_train, Y_trian, X_test, y_test\n=\nload_cifar_data(\n224\n,\n224\n)\nNext comes our inception module\nInception module contains 1x1 convolutions before 3x3 and 5x5 convolution operations. It takes different number of filters for different convolution operations and concatenate these operations to take into next layer.\ndef\ninception_module(x, filters_1x1, filters_3x3_reduce, filters_3x3, filters_5x5_reduce, filters_5x5, filters_pool_proj, name\n=\nNone\n):\nconv_1x1\n=\nConv2D(filters_1x1, (\n1\n,\n1\n), activation\n=\n'relu'\n, kernel_initializer\n=\nkernel_init, bias_initializer\n=\nbias_init)(x)\nconv_3x3\n=\nConv2D(filters_3x3_reduce, (\n1\n,\n1\n), padding\n=\n'same'\n, activation\n=\n'relu'\n, kernel_initializer\n=\nkernel_init, bias_initializer\n=\nbias_init)(x)\nconv_3x3\n=\nConv2D(filters_3x3, (\n3\n,\n3\n), padding\n=\n'same'\n, activation\n=\n'relu'\n, kernel_initializer\n=\nkernel_init, bias_initializer\n=\nbias_init)(conv_3x3)\nconv_5x5\n=\nConv2D(filters_5x5_reduce, (\n1\n,\n1\n), padding\n=\n'same'\n, activation\n=\n'relu'\n, kernel_initializer\n=\nkernel_init, bias_initializer\n=\nbias_init)(x)\nconv_5x5\n=\nConv2D(filters_5x5, (\n3\n,\n3\n), padding\n=\n'same'\n, activation\n=\n'relu'\n, kernel_initializer\n=\nkernel_init, bias_initializer\n=\nbias_init)(conv_5x5)\npool_proj\n=\nMaxPool2D((\n3\n,\n3\n), strides\n=\n(\n1\n,\n1\n), padding\n=\n'same'\n)(x)\npool_proj\n=\nConv2D(filters_pool_proj, (\n1\n,\n1\n), padding\n=\n'same'\n, activation\n=\n'relu'\n, kernel_initializer\n=\nkernel_init, bias_initializer\n=\nbias_init)(pool_proj)\noutput\n=\nconcatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis\n=\n3\n, name\n=\nname)\nreturn\noutput\nimport\ntensorflow\nkernel_init\n=\ntensorflow.keras.initializers.GlorotUniform()\nbias_init\n=\ntensorflow.initializers.Constant(value\n=\n0.2\n)\ninput_layer\n=\nInput(shape\n=\n(\n224\n,\n224\n,\n3\n))\nx\n=\nConv2D(\n64\n, (\n7\n,\n7\n), padding\n=\n'same'\n, strides\n=\n(\n2\n,\n2\n), activation\n=\n'relu'\n, name\n=\n'conv_1_7x7/2'\n, kernel_initializer\n=\nkernel_init, bias_initializer\n=\nbias_init)(input_layer)\nx\n=\nMaxPool2D((\n3\n,\n3\n), padding\n=\n'same'\n, strides\n=\n(\n2\n,\n2\n), name\n=\n'max_pool_1_3x3/2'\n)(x)\nx\n=\nConv2D(\n64\n, (\n1\n,\n1\n), padding\n=\n'same'\n, strides\n=\n(\n1\n,\n1\n), activation\n=\n'relu'\n, name\n=\n'conv_2a_3x3/1'\n, kernel_initializer\n=\nkernel_init, bias_initializer\n=\nbias_init)(x)\nx\n=\nConv2D(\n192\n, (\n3\n,\n3\n), padding\n=\n'same'\n, strides\n=\n(\n1\n,\n1\n), activation\n=\n'relu'\n, name\n=\n'conv_2b_3x3/1'\n, kernel_initializer\n=\nkernel_init, bias_initializer\n=\nbias_init)(x)\nx\n=\nMaxPool2D((\n3\n,\n3\n), padding\n=\n'same'\n, strides\n=\n(\n2\n,\n2\n), name\n=\n'max_pool_2_3x3/2'\n)(x)\nx\n=\ninception_module(x,\nfilters_1x1\n=\n64\n,\nfilters_3x3_reduce\n=\n96\n,\nfilters_3x3\n=\n128\n,\nfilters_5x5_reduce\n=\n16\n,\nfilters_5x5\n=\n32\n,\nfilters_pool_proj\n=\n32\n,\nname\n=\n'inception_3a'\n)\nx\n=\ninception_module(x,\nfilters_1x1\n=\n128\n,\nfilters_3x3_reduce\n=\n128\n,\nfilters_3x3\n=\n192\n,\nfilters_5x5_reduce\n=\n32\n,\nfilters_5x5\n=\n96\n,\nfilters_pool_proj\n=\n64\n,\nname\n=\n'inception_3b'\n)\nx\n=\nMaxPool2D((\n3\n,\n3\n), strides\n=\n(\n2\n,\n2\n), padding\n=\n'same'\n, name\n=\n'max_pool_3_3x3/2'\n)(x)\nx\n=\ninception_module(x,\nfilters_1x1\n=\n192\n,\nfilters_3x3_reduce\n=\n96\n,\nfilters_3x3\n=\n208\n,\nfilters_5x5_reduce\n=\n16\n,\nfilters_5x5\n=\n48\n,\nfilters_pool_proj\n=\n64\n,\nname\n=\n'inception_4a'\n)\nx1\n=\nAveragePooling2D((\n5\n,\n5\n), strides\n=\n3\n)(x)\nx1\n=\nConv2D(\n128\n, (\n1\n,\n1\n), padding\n=\n'same'\n, activation\n=\n'relu'\n)(x1)\nx1\n=\nFlatten()(x1)\nx1\n=\nDense(\n1024\n, activation\n=\n'relu'\n)(x1)\nx1\n=\nDropout(\n0.4\n)(x1)\nx1\n=\nDense(\n10\n, activation\n=\n'softmax'\n, name\n=\n'auxiliary_output_1'\n)(x1)\nx\n=\ninception_module(x,\nfilters_1x1\n=\n160\n,\nfilters_3x3_reduce\n=\n112\n,\nfilters_3x3\n=\n224\n,\nfilters_5x5_reduce\n=\n24\n,\nfilters_5x5\n=\n64\n,\nfilters_pool_proj\n=\n64\n,\nname\n=\n'inception_4b'\n)\nx\n=\ninception_module(x,\nfilters_1x1\n=\n128\n,\nfilters_3x3_reduce\n=\n128\n,\nfilters_3x3\n=\n256\n,\nfilters_5x5_reduce\n=\n24\n,\nfilters_5x5\n=\n64\n,\nfilters_pool_proj\n=\n64\n,\nname\n=\n'inception_4c'\n)\nx\n=\ninception_module(x,\nfilters_1x1\n=\n112\n,\nfilters_3x3_reduce\n=\n144\n,\nfilters_3x3\n=\n288\n,\nfilters_5x5_reduce\n=\n32\n,\nfilters_5x5\n=\n64\n,\nfilters_pool_proj\n=\n64\n,\nname\n=\n'inception_4d'\n)\nx2\n=\nAveragePooling2D((\n5\n,\n5\n), strides\n=\n3\n)(x)\nx2\n=\nConv2D(\n128\n, (\n1\n,\n1\n), padding\n=\n'same'\n, activation\n=\n'relu'\n)(x2)\nx2\n=\nFlatten()(x2)\nx2\n=\nDense(\n1024\n, activation\n=\n'relu'\n)(x2)\nx2\n=\nDropout(\n0.4\n)(x2)\nx2\n=\nDense(\n10\n, activation\n=\n'softmax'\n, name\n=\n'auxiliary_output_2'\n)(x2)\nx\n=\ninception_module(x,\nfilters_1x1\n=\n256\n,\nfilters_3x3_reduce\n=\n160\n,\nfilters_3x3\n=\n320\n,\nfilters_5x5_reduce\n=\n32\n,\nfilters_5x5\n=\n128\n,\nfilters_pool_proj\n=\n128\n,\nname\n=\n'inception_4e'\n)\nx\n=\nMaxPool2D((\n3\n,\n3\n), strides\n=\n(\n2\n,\n2\n), padding\n=\n'same'\n, name\n=\n'max_pool_4_3x3/2'\n)\nx\n=\ninception_module(x,\nfilters_1x1\n=\n256\n,\nfilters_3x3_reduce\n=\n160\n,\nfilters_3x3\n=\n320\n,\nfilters_5x5_reduce\n=\n32\n,\nfilters_5x5\n=\n128\n,\nfilters_pool_proj\n=\n128\n,\nname\n=\n'inception_5a'\n)\nx\n=\ninception_module(x,\nfilters_1x1\n=\n384\n,\nfilters_3x3_reduce\n=\n192\n,\nfilters_3x3\n=\n384\n,\nfilters_5x5_reduce\n=\n48\n,\nfilters_5x5\n=\n128\n,\nfilters_pool_proj\n=\n128\n,\nname\n=\n'inception_5b'\n)\nx\n=\nGlobalAveragePooling2D(name\n=\n'avg_pool_5_3x3/1'\n)(x)\nx\n=\nDropout(\n0.4\n)(x)\nx\n=\nDense(\n10\n, activation\n=\n'softmax'\n, name\n=\n'output'\n)(x)\nmodel\n=\nModel(input_layer, [x, x1, x2], name\n=\n'inception_v1'\n)\nGetting the summary of the model\nmodel.summary()\nepochs\n=\n25\ninitial_lrate\n=\n0.01\ndef\ndecay(epoch, steps\n=\n100\n):\ninitial_lrate\n=\n0.01\ndrop\n=\n0.96\nepochs_drop\n=\n8\nlrate\n=\ninitial_lrate\n*\nmath.\npow\n(drop, math.floor((\n1\n+\nepoch)\n/\nepochs_drop))\nreturn\nlrate\nsgd\n=\nSGD(lr\n=\ninitial_lrate, momentum\n=\n0.9\n, nesterov\n=\nFalse\n)\nlr_sc\n=\nLearningRateScheduler(decay, verbose\n=\n1\n)\nmodel.\ncompile\n(loss\n=\n[\n'categorical_crossentropy'\n,\n'categorical_crossentropy'\n,\n'categorical_crossentropy'\n], loss_weights\n=\n[\n1\n,\n0.3\n,\n0.3\n], optimizer\n=\nsgd, metrics\n=\n[\n'accuracy'\n])\nUsing our model to fit the training data\nhistory\n=\nmodel.fit(X_train, [y_train, y_train, y_train], validation_data\n=\n(X_test, [y_test, y_test, y_test]), epochs\n=\nepochs, batch_size\n=\n256\n, callbacks\n=\n[lr_sc])\nReferences\nhttps://www.analyticsvidhya.com/blog/2018/10/understanding-inception-network-from-scratch/\nGoing Deeper with Convolutions\nhttps://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202"
      ]
    },
    {
      "title": "VGGNet Architecture Explained \u2013 Prabin Nepal",
      "url": "https://nepalprabin.github.io/posts/2020-05-09-vggnet-architecture-explained.html",
      "relevance_score": 0.014343912713229656,
      "content": [
        "VGGNet Architecture Explained \u2013 Prabin Nepal\nVGGNet is a Convolutional Neural Network architecture proposed by Karen Simonyan and Andrew Zisserman of University of Oxford in 2014. This paper mailny focuses in the effect of the convolutional neural network depth on its accuracy. You can find the original paper of VGGNet which is titled as\nVery Deep Convolutional Networks for Large Scale Image Recognition\n.\nArchitecture\nThe input to VGG based convNet is a 224*224 RGB image. Preprocessing layer takes the RGB image with pixel values in the range of 0 - 255 and subtracts the mean image values which is calculated over the entire ImageNet training set.\nFig. A visualization of the VGG architecture (\nsource\n)\nThe input images after preprocessing are passed through these weight layers. The training images are passed through a stack of convolution layers. There are total of 13 convolutional layers and 3 fully connected layers in VGG16 architecture. VGG has smaller filters (3*3) with more depth instead of having large filters. It has ended up having the same effective receptive field as if you only have one 7 x 7 convolutional layers.\nAnother variation of VGGNet has 19 weight layers consisting of 16 convolutional layers with 3 fully connected layers and same 5 pooling layers. In both variation of VGGNet there consists of two Fully Connected layers with 4096 channels each which is followed by another fully connected layer with 1000 channels to predict 1000 labels. Last fully connected layer uses softmax layer for classification purpose.\nVGG 16 and VGG 19 Layers (\nsource\n)\nArchitecture walkthrough:\nThe first two layers are convolutional layers with 3x3 filters, and first two layers use 64 filters that results in 224x224x64 volume as same convolutions are used. The filters are always 3x3 with stride of 1\nAfter this, pooling layer was used with max-pool of 2x2 size and stride 2 which reduces height and width of a volume from 224x224x64 to 112x112x64.\nThis is followed by 2 more convolution layers with 128 filters. This results in the new dimension of 112x112x128.\nAfter pooling layer is used, volume is reduced to 56x56x128.\nTwo more convolution layers are added with 256 filters each followed by down sampling layer that reduces the size to 28x28x256.\nTwo more stack each with 3 convolution layer is separated by a max-pool layer.\nAfter the final pooling layer, 7x7x512 volume is flattened into Fully Connected (FC) layer with 4096 channels and softmax output of 1000 classes.\nImplementation\nNow let\u2019s go ahead and see how we can implement this architecture using\ntensorflow\n. This implementation is inspired from Machine Learning Tokyo\u2019s\nCNN architectures\n.\nImporting libraries\nStarting the convolution blocks with the input layer\ninput\n=\nInput(shape\n=\n(\n224\n,\n224\n,\n3\n))\n1st block consists of 2 convolution layer each with 64 filters of 3*3 and followed by a max-pool layer with stride 2 and pool-size of 2. All hidden layer uses ReLU for non-linearity.\nx\n=\nConv2D(filters\n=\n64\n, kernel_size\n=\n3\n, padding\n=\n'same'\n, activation\n=\n'relu'\n)(\ninput\n)\nx\n=\nConv2D(filters\n=\n64\n, kernel_size\n=\n3\n, padding\n=\n'same'\n, activation\n=\n'relu'\n)(x)\nx\n=\nMaxPool2D(pool_size\n=\n2\n, strides\n=\n2\n, padding\n=\n'same'\n)(x)\n2nd block also consists of 2 convolution layer each with 128 filters of 3*3 and followed by a max-pool layer with stride 2 and pool-size of 2.\nx\n=\nConv2D(filters\n=\n128\n, kernel_size\n=\n3\n, padding\n=\n'same'\n, activation\n=\n'relu'\n)(x)\nx\n=\nConv2D(filters\n=\n128\n, kernel_size\n=\n3\n, padding\n=\n'same'\n, activation\n=\n'relu'\n)(x)\nx\n=\nMaxPool2D(pool_size\n=\n2\n, strides\n=\n2\n, padding\n=\n'same'\n)(x)\n3rd block consists of 3 convolution layer each with 256 filters of 3*3 and followed by a max-pool layer with stride 2 and pool-size of 2.\nx\n=\nConv2D(filters\n=\n256\n, kernel_size\n=\n3\n, padding\n=\n'same'\n, activation\n=\n'relu'\n)(x)\nx\n=\nConv2D(filters\n=\n256\n, kernel_size\n=\n3\n, padding\n=\n'same'\n, activation\n=\n'relu'\n)(x)\nx\n=\nConv2D(filters\n=\n256\n, kernel_size\n=\n3\n, padding\n=\n'same'\n, activation\n=\n'relu'\n)(x)\nx\n=\nMaxPool2D(pool_size\n=\n2\n, strides\n=\n2\n, padding\n=\n'same'\n)(x)\n4th and 5th block consists of 3 convolutional layers with 512 filters each. In between these blocks, a max-pool layer is used with stride of 2 and pool-size of 2.\nx\n=\nConv2D(filters\n=\n512\n, kernel_size\n=\n3\n, padding\n=\n'same'\n, activation\n=\n'relu'\n)(x)\nx\n=\nConv2D(filters\n=\n512\n, kernel_size\n=\n3\n, padding\n=\n'same'\n, activation\n=\n'relu'\n)(x)\nx\n=\nConv2D(filters\n=\n512\n, kernel_size\n=\n3\n, padding\n=\n'same'\n, activation\n=\n'relu'\n)(x)\nx\n=\nMaxPool2D(pool_size\n=\n2\n, strides\n=\n2\n, padding\n=\n'same'\n)(x)\nx\n=\nConv2D(filters\n=\n512\n, kernel_size\n=\n3\n, padding\n=\n'same'\n, activation\n=\n'relu'\n)(x)\nx\n=\nConv2D(filters\n=\n512\n, kernel_size\n=\n3\n, padding\n=\n'same'\n, activation\n=\n'relu'\n)(x)\nx\n=\nConv2D(filters\n=\n512\n, kernel_size\n=\n3\n, padding\n=\n'same'\n, activation\n=\n'relu'\n)(x)\nx\n=\nMaxPool2D(pool_size\n=\n2\n, strides\n=\n2\n, padding\n=\n'same'\n)(x)\nThe output from 5th convolution block is Flattened which gives 4096 units. This fully connected layer is connected to another FC layer having same number of units. The final fully connected layer contains 1000 units and softmax activation which is used for classification of 1000 classes\n#Dense Layers\nx\n=\nFlatten(x)\nx\n=\nDense(units\n=\n4096\n, activation\n=\n'relu'\n)(x)\nx\n=\nDense(units\n=\n4096\n, activation\n=\n'relu'\n)(x)\noutput\n=\nDense(units\n=\n1000\n, activation\n=\n'softmax'\n)(x)\nfrom\ntensorflow.keras\nimport\nModel\nmodel\n=\nModel(inputs\n=\ninput\n, outputs\n=\noutput)"
      ]
    }
  ]
}