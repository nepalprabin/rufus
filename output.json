{
  "title": "Information about: Extract all the blog contents",
  "prompt": "Extract all the blog contents",
  "sources": [
    "https://nepalprabin.github.io/posts/2022-10-19-text-summarization-nlp.html",
    "https://nepalprabin.github.io/posts/2025-03-02-huggingface-smolagents-solutions.html",
    "https://nepalprabin.github.io/posts/2023-07-04-augmented-language-models.html",
    "https://nepalprabin.github.io",
    "https://nepalprabin.github.io/index.html",
    "https://github.com/nepalprabin"
  ],
  "created_at": "2025-03-05T17:10:47.343871",
  "sections": [
    {
      "title": "Text Summarization NLP \u2013 Prabin Nepal",
      "url": "https://nepalprabin.github.io/posts/2022-10-19-text-summarization-nlp.html",
      "relevance_score": 0.20714044570922852,
      "content": [
        "Text Summarization NLP \u2013 Prabin Nepal\nWhat is text summarization?\nText summarization is one of the Natural Language Processing (NLP) tasks where documents/texts are shortened automatically while holding the same semantic meaning. Summarization process generates short, fluent and accurate summary of the long documents. The main idea of text summarization is to find the subset of the most important information from the entire document and present it in a human readable format. Text summarization has its application in other NLP tasks such as Question Answering (QA), Text Classification, Text Generation and other fields.\nTypes of summarization\nBased on how the texts are extracted from the documents, the summarization process can be divided into two types: extractive summarization and abstractive summarization.\nExtractive Summarization\nExtractive summarization picks up the most important sentences directly from the documents and forms a coherent summary. This is done using a scoring function. Extractive summarization takes a sentence as an input and produces a probability vector as the output. This probability vector represents the probability of a sentence being included in the summary.\nImplementing extractive summarization based on word frequency\nWe can implement extractive summarization using word frequency in five simple steps:\na. Creating word frequency table\nWe count the frequency of the words present in the text and create a frequency table which is a dictionary to store the count. While creating the frequency table, we do not account for the stop words present in the text and remove those words.\ndef\nfrequency_table(text):\n# all unique stopwords of english\nstop_words\n=\nset\n(stopwords.words(\n\"english\"\n))\nwords\n=\nword_tokenize(text)\nfreq_table\n=\ndict\n()\n# creating frequency table to keep the count of each word\nfor\nword\nin\nwords:\nword\n=\nword.lower()\nif\nword\nin\nstop_words:\ncontinue\nif\nword\nin\nfreq_table:\nfreq_table[word]\n+=\n1\nelse\n:\nfreq_table[word]\n=\n1\nreturn\nfreq_table\nb. Tokenizing the sentences\nHere we tokenize the sentences using NLTK\u2019s sent_tokenize() method. This separates paragraphs into individual sentences.\ndef\ntokenize_sentence(text):\nreturn\nsent_tokenize(text)\nc.\u00a0Scoring the sentences using term frequency\nHere, we score a sentence by its words, by adding frequency of every word present in the sentence excluding stop words. One downside of this approach is, if the sentence is long, the value of frequency increases.\ndef\nterm_frequency_score(sentence, freq_table):\n# dictionary to keep the score\nsentence_value\n=\ndict\n()\nfor\nsentence\nin\nsentences:\nfor\nword, freq\nin\nfreq_table.items():\nif\nword\nin\nsentence.lower():\nif\nsentence\nin\nsentence_value:\nsentence_value[sentence]\n+=\nfreq\nelse\n:\nsentence_value[sentence]\n=\nfreq\nreturn\nsentence_value\nd.\u00a0Finding the threshold score\nAfter calculating the term frequency, we calculate the threshold score.\ndef\ncalculate_average_score(sentence_value):\n# To compare the sentences within the text, we assign a score.\nsum_values\n=\n0\nfor\nsentence\nin\nsentence_value:\nsum_values\n+=\nsentence_value[sentence]\n# Calculating average score of the sentence. This average score can be a good threshold.\naverage\n=\nint\n(sum_values\n/\nlen\n(sentence_value))\nreturn\naverage\ne. Generating the summary based on the threshold value\nBased on the threshold value, we generate the summary of the text.\ndef\ncreate_summary(sentences, sentence_value, threshold):\n# Applying the threshold value and storing sentences in an order into the summary.\nsummary\n=\n''\nfor\nsentence\nin\nsentences:\nif\n(sentence\nin\nsentence_value)\nand\n(sentence_value[sentence]\n>\n(\n1.2\n*\nthreshold)):\nsummary\n+=\n\" \"\n+\nsentence\nreturn\nsummary\nAbstractive Summarization\nIn abstractive summarization, the model forms its own phrases and sentences to provide a consistent summary. Abstractive summarization does not simply copy the sentences to form the summary but create new phrases that are relevant to the original document. This summarization technique uses deep learning techniques (like seq2seq) to paraphrase and shorten the original document.\nAbstractive Summarization using Transformers\nTransformers is an architecture which uses attention mechanisms to solve sequence to sequence problems while solving long term dependencies. Ever since it was introduced in 2017, transformers have been widely used in various NLP tasks such as text generation, question answering, text classification, language translation and so on. The transformer architecture consists of encoder and decoder parts. The encoder component consists of 6 encoders each of which consists of two sub layers: self-attention and feed forward networks. The input text is first converted into vectors using text embedding methods. Then the vector is passed into the self attention layer and the output from the self attention layer is passed through the feed forward network. The decoder also consists of both self attention and feed forward network layer. An additional layer is present in between these components which is an attention layer that helps the decoder to focus on the relevant parts of the input sentence.\nFig. Transformer architecture (from original paper)\nHuggingface Transformers provide various pre-trained models to perform NLP tasks. It provides APIs and tools to download and train state-of-the-art pre-trained models. Not only NLP, huggingface supports Computer Vision tasks like image classification, object detection and segmentation, audio classification and recognition, and multimodal tasks like table question answering, optical character recognition, and many more.\nBasic transformer pipeline for summarization\nHuggingface transformers provide an easy to use model for inference using pipeline. These pipelines are the objects that hide complex code and provide a simple API to perform various tasks.\nfrom\ntransformers\nimport\npipeline\nclassifier\n=\npipeline(\n\"summarization\"\n)\ntext\n=\n\"\"\"Acnesol Gel is an antibiotic that fights bacteria. It is used to treat acne, which appears as spots or pimples on your face, chest or back. This medicine works by attacking the bacteria that cause these pimples.Acnesol Gel is only meant for external use and should be used as advised by your doctor. You should normally wash and dry the affected area before applying a thin layer of the medicine. It should not be applied to broken or damaged skin.  Avoid any contact with your eyes, nose, or mouth. Rinse it off with water if you accidentally get it in these areas. It may take several weeks for your symptoms to improve, but you should keep using this medicine regularly. Do not stop using it as soon as your acne starts to get better. Ask your doctor when you should stop treatment. Common side effects like minor itching, burning, or redness of the skin and oily skin may be seen in some people. These are usually temporary and resolve on their own. Consult your doctor if they bother you or do not go away.It is a safe medicine, but you should inform your doctor if you have any problems with your bowels (intestines). Also, inform the doctor if you have ever had bloody diarrhea caused by taking antibiotics or if you are using any other medicines to treat skin conditions. Consult your doctor about using this medicine if you are pregnant or breastfeeding.\"\"\"\nclassifier(text)\nResult:\n[{\n'summary_text'\n:\n' Acnesol Gel is an antibiotic that fights bacteria that causes pimples . It is used to treat acne, which appears as spots or pimples on your face, chest or back . The medicine is only meant for external use and should be used as advised by your doctor .'\n}]\nThe\npipeline()\ntakes the name of the task to be performed (if we want to perform a question-answering task, then we can simply pass \u201cquestion-answering\u201d into the pipeline() and it automatically loads the model to perform the specific task.\nFine-tuning summarization model for medical dataset\nSummarization using abstractive technique is hard as compared to extractive summarization as we need to generate new text as the output. Different architectures like GTP, T5, BART are used to perform summarization tasks. We will be using the PubMed dataset. It contains datasets of long and structured documents obtained from PubMed OpenAccess repositories. from datasets import load_dataset\npubmed\n=\nload_dataset(\n\"ccdv/pubmed-summarization\"\n)\nThe PubMed dataset contains article, abstract and section_names as columns. The first step after loading the dataset is tokenizing the training data. Tokenization is the process of splitting paragraphs, sentences into smaller units called tokens. tokenizer = AutoTokenizer.from_pretrained(\u2018facebook/bart-large-cnn\u2019)\nThe next step is to preprocess the data. Before training the data, we need to convert our data into expected model input format.\ndef\npreprocess_function(examples):\ninputs\n=\n[doc\nfor\ndoc\nin\nexamples[\n\"article\"\n]]\nmodel_inputs\n=\ntokenizer(inputs, max_length\n=\n1024\n, truncation\n=\nTrue\n)\nlabels\n=\ntokenizer(examples[\n\"abstract\"\n], max_length\n=\n128\n, truncation\n=\nTrue\n, padding\n=\nTrue\n)\nmodel_inputs[\n\"labels\"\n]\n=\nlabels[\n\"input_ids\"\n]\nreturn\nmodel_inputs\nWe need to apply the processing function over the entire dataset. Setting flag\nbatched=True\nhelps to speed up the processing of multiple elements of the dataset at once.\ntokenized_pubmed\n=\npubmed.\nmap\n(preprocess_function, batched\n=\nTrue\n)\nNext, we need to create a batch for all the examples. Huggingface provides a data collator to create a batch for the examples.\ntokenized_datasets\n=\ntokenized_pubmed.remove_columns(pubmed[\n\"train\"\n].column_names)\ndata_collator\n=\nDataCollatorForSeq2Seq(tokenizer\n=\ntokenizer, model\n=\n\"facebook/bart-large-cnn\"\n)\nHuggingface provides various pre-trained models that we can leverage to perform a variety of machine learning tasks.\nmodel\n=\nAutoModelForSeq2SeqLM.from_pretrained(model)\nBefore training the model, we need to define our training hyperparamaters using training arguments. Since text summarization is a sequence to sequence tasks, we are using Seq2SeqTrainingArguments. And, we need to define our trainer by passing training and test dataset along with training arguments.\n# training arguments\ntraining_arguments\n=\nSeq2SeqTrainingArguments(\noutput_dir\n=\n'./results'\n,\nevaluation_strategy\n=\n'epoch'\n,\nlearning_rate\n=\n2e-5\n,\nper_device_train_batch_size\n=\n8\n,\nper_device_eval_batch_size\n=\n8\n,\nweight_decay\n=\n0.01\n,\nsave_total_limit\n=\n3\n,\nnum_train_epochs\n=\n3\n,\n# remove_unused_columns=False,\n# fp16=True,\n)\ntrainer\n=\nSeq2SeqTrainer(\nmodel\n=\nmodel,\nargs\n=\ntraining_arguments,\ntrain_dataset\n=\ntokenized_pubmed[\n'train'\n],\neval_dataset\n=\ntokenized_pubmed[\n'validation'\n],\ntokenizer\n=\ntokenizer,\ndata_collator\n=\ndata_collator\n)\nThe last step is to call\ntrain()\nto fine-tune our model.\ntrainer.train()\nConclusion\nSummarization helps to generalize the long documents by paraphrasing the important sentences from the whole document. It is very helpful in various applications like summarizing legal contracts, medical documents, news information and many more."
      ]
    },
    {
      "title": "Huggingface AI Agents Quiz Solutions \u2013 Prabin Nepal",
      "url": "https://nepalprabin.github.io/posts/2025-03-02-huggingface-smolagents-solutions.html",
      "relevance_score": 0.13337460160255432,
      "content": [
        "Huggingface AI Agents Quiz Solutions \u2013 Prabin Nepal\nI have been diving into AI agents through Huggingface\u2019s AI Agents Course. This course offers a comprehensive understanding of how to build and deploy AI agents using the\nsmolagents\nlibrary. In this blog, I\u2019ll share insights from the course (Unit 2) and provide code snippets to illustrate key concepts.\nNote\nHere is the course link if anyone is interested.\nAI Agents Course\nCreate a Basic Code Agent with Web Search Capability\nOne of the foundational exercises involves creating a CodeAgent equipped with web search capabilities. This agent leverages the DuckDuckGoSearchTool to perform web searches, enabling it to fetch real-time information. Here\u2019s how you can set it up:\n# Create a CodeAgent with DuckDuckGo search capability\nfrom\nsmolagents\nimport\nCodeAgent, DuckDuckGoSearchTool, HfApiModel\nagent\n=\nCodeAgent(\ntools\n=\n[DuckDuckGoSearchTool()],\n# Add search tool here\nmodel\n=\nHfApiModel(\n\"Qwen/Qwen2.5-Coder-32B-Instruct\"\n)\n# Add model here\n)\nIn this snippet, we initialize a CodeAgent with the DuckDuckGoSearchTool, allowing the agent to perform web searches to answer queries.\nSet Up a Multi-Agent System with Manager and Web Search Agents\nMulti-Agent systems are the agents that are specialized on complex tasks with more scalable and robust nature. In\nsmolagents\n, various agents can be integrated to produce Python code, invoke external tools, conduct web searches, and more. By coordinating these agents, it\u2019s possible to develop robust workflows. A typical multi-agent system includes:\n- A manager Agent\n- A code interpreter Agent\n- A web Search Agent\nMulti-agent system allows to separate memories between different sub-tasks and provide great benefits. Firstly, each agent are more focused on its core taks and secondly, separating memories reduces the count of input tokens resulting in reducing latency and cost. Below is the multi-agent system when\nweb_agent\nperforms search and\nmanager_agent\ngives data analysis capabilities. Also, we can import dependencies (like python libraries) that helps to perform the tasks.\nfrom\nsmolagents\nimport\nCodeAgent, ToolCallingAgent, DuckDuckGoSearchTool, HfApiModel, VisitWebpageTool\nweb_agent\n=\nToolCallingAgent(\ntools\n=\n[DuckDuckGoSearchTool(), VisitWebpageTool()],\nmodel\n=\nHfApiModel(model_id\n=\n\"Qwen/Qwen2.5-Coder-32B-Instruct\"\n),\nmax_steps\n=\n10\n,\nname\n=\n\"search\"\n,\ndescription\n=\n\"Agent to perform web searches and visit webpages.\"\n)\nmanager_agent\n=\nCodeAgent(\nmodel\n=\nHfApiModel(model_id\n=\n\"Qwen/Qwen2.5-Coder-32B-Instruct\"\n),\nmanaged_agents\n=\n[web_agent],\nadditional_authorized_imports\n=\n[\n\"pandas\"\n,\n\"time\"\n,\n\"numpy\"\n]\n# Corrected imports\n)\nConfigure Agent Security Settings\nSecurity is a crucial aspect when deploying AI agents, especially when they execute code. Below code snippet uses E2B to run code in a sandboxed environment. It is a remote execution that run the code in a isolated container.\nfrom\nsmolagents\nimport\nCodeAgent, HfApiModel\nfrom\nsmolagents.sandbox\nimport\nE2BSandbox\nmodel\n=\nHfApiModel(\n\"Qwen/Qwen2.5-Coder-32B-Instruct\"\n)\nagent\n=\nCodeAgent(\ntools\n=\n[],\nmodel\n=\nmodel,\nsandbox\n=\nE2BSandbox(),\n# Configure the sandbox\nadditional_authorized_imports\n=\n[\n\"numpy\"\n],\n# Authorize numpy import\n)\nImplement a Tool-Calling Agent\nSimilar to\nCodeAgent\n,\nToolCallingAgent\nis another type of agent available in smolagent library. CodeAgent uses Python code snippets whereas ToolCallingAgent use built-in tool-calling capabilities of LLM providers and generate JSON structures.\nfrom\nsmolagents\nimport\nToolCallingAgent, HfApiModel, DuckDuckGoSearchTool\nagent\n=\nToolCallingAgent(\ntools\n=\n[DuckDuckGoSearchTool()],\nmodel\n=\nHfApiModel(model_id\n=\n\"Qwen/Qwen2.5-Coder-32B-Instruct\"\n),\nname\n=\n\"SearchAgent\"\n,\ndescription\n=\n\"An agent that uses DuckDuckGo to search the web.\"\n,\nmax_steps\n=\n5\n,\n)\nSet Up Model Integration\nLLM models are the most important aspect when creating AI agents. There are many model availables for various tasks and domains. So we can easily integrate models that is required for our task. Below code snippet switches between two different models providers.\nfrom\nsmolagents\nimport\nHfApiModel, LiteLLMModel\n# Initialize Hugging Face model\nhf_model\n=\nHfApiModel(model_id\n=\n\"Qwen/Qwen2.5-Coder-32B-Instruct\"\n)\n# Initialize LiteLLM model as an alternative model\nother_model\n=\nLiteLLMModel(model_id\n=\n\"anthropic/claude-3-sonnet\"\n)\n# Set the model to hf_model or alternative model\nmodel\n=\nhf_model\n# Alternatively, you can switch this to `other_model`"
      ]
    },
    {
      "title": "Augmenting Large Language Models: Expanding Context and Enhancing Relevance \u2013 Prabin Nepal",
      "url": "https://nepalprabin.github.io/posts/2023-07-04-augmented-language-models.html",
      "relevance_score": 0.13185574114322662,
      "content": [
        "Augmenting Large Language Models: Expanding Context and Enhancing Relevance \u2013 Prabin Nepal\nWith the rise of ChatGPT and other large language models (LLMs), the potential for AI to surpass human capabilities has become a topic of both fascination and concern. While LLMs excel at understanding language, following instructions, and reasoning, they often fall short when it comes to performing specific tasks. Simply inputting a prompt into ChatGPT may result in answers that are unrelated or out of context, a phenomenon known as \u201challucination.\u201d To obtain relevant information, it is crucial to provide the model with the appropriate context. However, the size of the context window is limited, posing a challenge in capturing all necessary information. Although the context size has increased over time, storing extensive information within a fixed context window remains impractical and expensive. This is where the augmentation of language models comes into play.\nAugmenting large language models involves three primary approaches:\nretrieval,\nchains, and\ntools.\nThese methods aim to enhance the capabilities of LLMs by providing them with additional resources and functionalities.\nRetrieval Augmentation:\nRetrieval augmentation involves leveraging an external corpus of data for the language model to search through. Traditionally, retrieval algorithms employ queries to rank relevant objects in a collection, which can include images, texts, documents, or other types of data. To enable efficient searching, the documents and their corresponding features are organized within an index. This index maps each feature to the documents containing it, facilitating quick retrieval. Boolean search determines the relevance of documents based on the query, while ranking is typically performed using algorithms like BM25 (Best Match 25).\nBM25 (Best Match 25) is a ranking function commonly used in information retrieval to measure the relevance of a document to a given query. It is a probabilistic retrieval model that enhances the vector space model by incorporating document length normalization and term frequency saturation.\nIn BM25, the indexing process involves tokenizing each document in the collection into terms and calculating term statistics such as document frequency (df) and inverse document frequency (idf). Document frequency represents the number of documents in the collection containing a particular term, while inverse document frequency measures the rarity of the term across the collection.\nDuring the querying phase, the query is tokenized into terms, and term statistics, including query term frequency (qtf) and query term inverse document frequency (qidf), are computed. These statistics capture the occurrence and relevance of terms in the query.\nWhile traditional retrieval methods primarily rely on keyword matching and statistical techniques, modern approaches leverage AI-centric retrieval methods that utilize embeddings. These methods offer improved search capabilities and help retrieve contextually relevant information.\nChains\nChains involve using the output of one language model as the input for another. By cascading multiple models together, the output of each model becomes the input for the subsequent one. This chaining process allows the models to build upon each other\u2019s knowledge and reasoning abilities, potentially leading to more accurate and contextually appropriate responses.\nThe sequential arrangement of models in a chain creates a pipeline of of interconnected language models, where the output of one model serves as the input for the next. This pipeline allows for a cascading flow of information and reasoning, enabling the models to collectively enhance their understanding and generate more accurate responses. By leveraging a chain of language models, each model can contribute its specialized knowledge and capabilities to the overall task. For example, one model may excel at language comprehension, while another may possess domain-specific knowledge.\nAs the input passes through the chain, each model can refine and expand upon the information, leading to a more comprehensive and contextually relevant output. The chaining process in language models has the potential to address the limitations of individual models, such as hallucination or generating irrelevant responses. By combining the strengths of multiple models, the pipeline can help mitigate these issues and produce more reliable and accurate results.\nFurthermore, the pipeline can be customized and tailored to specific use cases or tasks. Different models can be integrated into the chain based on their strengths and compatibility with the desired objectives. This flexibility allows for the creation of powerful and specialized systems that leverage the collective intelligence of multiple language models.\nLangchain\nLangchain\nhas emerged as an immensely popular tool for constructing chains of language models, making it one of the fastest-growing open-source projects in this domain. With support for both Python and JavaScript, it provides a versatile platform for building applications and can be seamlessly integrated into production environments. Langchain serves as the fastest way to kickstart development and offers a wide range of pre-built chains tailored for various tasks. Many developers find inspiration from Langchain and end up creating their own customized chaining solutions. One of the key strengths of Lang chain lies in its extensive repository, which houses numerous examples of different chaining patterns. These examples not only facilitate idea generation but also serve as valuable resources for learning and gaining insights into effective chaining techniques. Whether for rapid prototyping or constructing production-grade systems, Lang chain strikes a balance between ease of use and flexibility, empowering developers to effortlessly create their own chaining systems when needed.\nThe building block of Langchain are chains. Chains can be simple/generic or specialized. One simple chain is a generic chain that contains a single LLM. Generic chain takes a prompt and uses LLM for text generation based on the prompt. Let\u2019s see how to achieve a simple chain using OpenAI\u2019s gpt-3.5 turbo model.\nimport\nos\nos.environ[\n\"OPENAI_API_KEY\"\n]\n=\n\"...\"\nfrom\nlangchain.prompts\nimport\nPromptTemplate\ntemplate\n=\n\"\"\"\nWho won the oscar for the best actor in  a leading role on\n{year}\n?\n\"\"\"\nprompt\n=\nPromptTemplate(\ninput_variables\n=\n[\n\"year\"\n],\ntemplate\n=\ntemplate,\n)\nprint\n(prompt.\nformat\n(year\n=\n2012\n))\nOutput: Who won the oscar\nfor\nthe best actor\nin\na leading role on\n2012\n?\nPromptTemplate\nhelps to design prompt for your tasks and you can provide input variables if you want like below:\ntemplate\n=\n\"\"\"\nWho won the oscar for the best\n{role}\non\n{year}\n?\n\"\"\"\nWhile creating a prompt template for multiple variables, you need to pass all those variables in\ninput_variables\nargument\nprompt\n=\nPromptTemplate(\ninput_variables\n=\n[\n\"role\"\n,\n\"year\"\n],\ntemplate\n=\ntemplate,\n)\nTools\nThe another way to give LLMs access to outside world is to let them use tools.\nUsing Tools in Langchain\nTools are the flexible way to augment language model with external data. There are two ways to build tools into language models. First way is to manually create chains whereas the later one is the use of plugins and letting the model figure it out. Some example tools that can be use includes Arxiv, Bash, Bing Search, Google, etc.\nTools can be used in langchain using following code snippet (in Python):\nfrom\nlangchain.agents\nimport\nload_tools\ntool_names\n=\n[...]\ntools\n=\nload_tools(tool_names)\ntools\nYou can name the tools that you are going to use and load them using load_tools methods\nLet\u2019s use Python\u2019s requests module as a tool to extract data from the web\nfrom\nlangchain.agents\nimport\nload_tools\ntool_names\n=\n[\n'requests_all'\n]\nrequests_tools\n=\nload_tools(tool_names)\nrequests_tools\nOutput:\n[\nRequestsGetTool(name\n=\n'requests_get'\n, description\n=\n'A portal to the internet. Use this when you need to get specific content from a website. Input should be a  url (i.e. https://www.google.com). The output will be the text response of the GET request.'\n, args_schema\n=\nNone\n, return_direct\n=\nFalse\n, verbose\n=\nFalse\n, callbacks\n=\nNone\n, callback_manager\n=\nNone\n, handle_tool_error\n=\nFalse\n, requests_wrapper\n=\nTextRequestsWrapper(headers\n=\nNone\n, aiosession\n=\nNone\n)),\nRequestsPostTool(name\n=\n'requests_post'\n, description\n=\n'Use this when you want to POST to a website.\n\\n\nInput should be a json string with two keys: \"url\" and \"data\".\n\\n\nThe value of \"url\" should be a string, and the value of \"data\" should be a dictionary of\n\\n\nkey-value pairs you want to POST to the url.\n\\n\nBe careful to always use double quotes for strings in the json string\n\\n\nThe output will be the text response of the POST request.\n\\n\n'\n, args_schema\n=\nNone\n, return_direct\n=\nFalse\n, verbose\n=\nFalse\n, callbacks\n=\nNone\n, callback_manager\n=\nNone\n, handle_tool_error\n=\nFalse\n, requests_wrapper\n=\nTextRequestsWrapper(headers\n=\nNone\n, aiosession\n=\nNone\n)),\nRequestsPatchTool(name\n=\n'requests_patch'\n, description\n=\n'Use this when you want to PATCH to a website.\n\\n\nInput should be a json string with two keys: \"url\" and \"data\".\n\\n\nThe value of \"url\" should be a string, and the value of \"data\" should be a dictionary of\n\\n\nkey-value pairs you want to PATCH to the url.\n\\n\nBe careful to always use double quotes for strings in the json string\n\\n\nThe output will be the text response of the PATCH request.\n\\n\n'\n, args_schema\n=\nNone\n, return_direct\n=\nFalse\n, verbose\n=\nFalse\n, callbacks\n=\nNone\n, callback_manager\n=\nNone\n, handle_tool_error\n=\nFalse\n, requests_wrapper\n=\nTextRequestsWrapper(headers\n=\nNone\n, aiosession\n=\nNone\n)),\nRequestsPutTool(name\n=\n'requests_put'\n, description\n=\n'Use this when you want to PUT to a website.\n\\n\nInput should be a json string with two keys: \"url\" and \"data\".\n\\n\nThe value of \"url\" should be a string, and the value of \"data\" should be a dictionary of\n\\n\nkey-value pairs you want to PUT to the url.\n\\n\nBe careful to always use double quotes for strings in the json string.\n\\n\nThe output will be the text response of the PUT request.\n\\n\n'\n, args_schema\n=\nNone\n, return_direct\n=\nFalse\n, verbose\n=\nFalse\n, callbacks\n=\nNone\n, callback_manager\n=\nNone\n, handle_tool_error\n=\nFalse\n, requests_wrapper\n=\nTextRequestsWrapper(headers\n=\nNone\n, aiosession\n=\nNone\n)),\nRequestsDeleteTool(name\n=\n'requests_delete'\n, description\n=\n'A portal to the internet. Use this when you need to make a DELETE request to a URL. Input should be a specific url, and the output will be the text response of the DELETE request.'\n, args_schema\n=\nNone\n, return_direct\n=\nFalse\n, verbose\n=\nFalse\n, callbacks\n=\nNone\n, callback_manager\n=\nNone\n, handle_tool_error\n=\nFalse\n, requests_wrapper\n=\nTextRequestsWrapper(headers\n=\nNone\n, aiosession\n=\nNone\n))\n]\nEach tool inside the\nrequest_all\ntool contains a request wapper. We can directly work with these wrappers as below:\nrequests_tools[\n0\n].requests_wrapper\nOutput:\nTextRequestsWrapper(headers\n=\nNone\n, aiosession\n=\nNone\n)\nWe can use\nTextRequestsWrapper\nto create a request object and use the object to extract data from the web.\nfrom\nlangchain.utilities\nimport\nTextRequestsWrapper\nrequests\n=\nTextRequestsWrapper()\nrequests.get(\n\"https://reqres.in/api/users?page=2\"\n)\nOutput:\n'{\"page\":2,\"per_page\":6,\"total\":12,\"total_pages\":2,\"data\":[{\"id\":7,\"email\":\"michael.lawson@reqres.in\",\"first_name\":\"Michael\",\"last_name\":\"Lawson\",\"avatar\":\"https://reqres.in/img/faces/7-image.jpg\"},{\"id\":8,\"email\":\"lindsay.ferguson@reqres.in\",\"first_name\":\"Lindsay\",\"last_name\":\"Ferguson\",\"avatar\":\"https://reqres.in/img/faces/8-image.jpg\"},{\"id\":9,\"email\":\"tobias.funke@reqres.in\",\"first_name\":\"Tobias\",\"last_name\":\"Funke\",\"avatar\":\"https://reqres.in/img/faces/9-image.jpg\"},{\"id\":10,\"email\":\"byron.fields@reqres.in\",\"first_name\":\"Byron\",\"last_name\":\"Fields\",\"avatar\":\"https://reqres.in/img/faces/10-image.jpg\"},{\"id\":11,\"email\":\"george.edwards@reqres.in\",\"first_name\":\"George\",\"last_name\":\"Edwards\",\"avatar\":\"https://reqres.in/img/faces/11-image.jpg\"},{\"id\":12,\"email\":\"rachel.howell@reqres.in\",\"first_name\":\"Rachel\",\"last_name\":\"Howell\",\"avatar\":\"https://reqres.in/img/faces/12-image.jpg\"}],\"support\":{\"url\":\"https://reqres.in/#support-heading\",\"text\":\"To keep ReqRes free, contributions towards server costs are appreciated!\"}}'\nReferences\nFull Stack Deep Learning (LLM Bootcamp)\nLangchain"
      ]
    },
    {
      "title": "Prabin Nepal",
      "url": "https://nepalprabin.github.io",
      "relevance_score": 0.12042945623397827,
      "content": [
        "Prabin Nepal\nCategories\nAll\n(16)\nLLM\n(1)\nNLP\n(4)\nagents\n(1)\ncomputer-vision\n(6)\ndeep-learning\n(12)\nllms\n(1)\nmachine-learning\n(2)\nself-supervised-learning\n(1)\nHuggingface AI Agents Quiz Solutions\nllms\nagents\nI have been diving into AI agents through Huggingface\u2019s AI Agents Course. This course offers a comprehensive understanding of how to build and deploy AI agents using the\nsmol\u2026\nMar 2, 2025\nAugmenting Large Language Models: Expanding Context and Enhancing Relevance\nmachine-learning\nNLP\ndeep-learning\nLLM\nWith the rise of ChatGPT and other large language models (LLMs), the potential for AI to surpass human capabilities has become a topic of both fascination and concern. While\u2026\nJul 4, 2023\nBrief overview of GPT-4\nmachine-learning\nNLP\ndeep-learning\nSince the release of ChatGPT, there has been significant interest and discussion within the broader AI and natural language processing communities regarding its\u2026\nMar 15, 2023\nText Summarization NLP\nNLP\ndeep-learning\nText summarization is one of the Natural Language Processing (NLP) tasks where documents/texts are shortened automatically while holding the same semantic meaning.\u2026\nOct 19, 2022\nAutocorrect and Minimum Edit Distance\nNLP\ndeep-learning\nThis is my brief note from\nDeepLearning.AI\u2019s\nNLP Specialization Course.\nOct 25, 2021\nIllustrated Vision Transformers\ncomputer-vision\ndeep-learning\nEver since Transformer was introduced in 2017, there has been a huge success in the field of Natural Language Processing (NLP). Almost all NLP tasks use Transformers and\u2026\nJul 27, 2021\nPaper Explanation: A Simple Framework for Contrastive Learning of Visual Representations (simCLR)\nVarious self-supervised learning methods have been proposed in recent years for learning image representations. Though a lot of methods have been proposed, the performance\u2026\nMar 26, 2021\nDeep Residual Learning for Image Recognition (ResNet paper explained)\nDeep Neural Networks tend to provide more accuracy as the number of layers increases. But, as we go more deeper in the network, the accuracy of the network decreases instead\u2026\nJan 1, 2021\nSelf-supervised Learning\ndeep-learning\nself-supervised-learning\nI have been exploring self-supervised learning and been through papers and blogs to understand it. Self-supervised learning is considered the next big thing in deep learning\u2026\nDec 8, 2020\nMobileNet Architecture Explained\ndeep-learning\nIn this blog post, I will try to write about the MobileNets and its architecture. MobileNet uses depthwise separable convolutions instead of standard convolution to reduce\u2026\nSep 21, 2020\nNeural style transfer and its working\nHave you ever used an app called Prisma that styles your image using popular paintings and turns your photo stunning? If that\u2019s the case then, the app you are using is the\u2026\nAug 23, 2020\nDeep Convolutional Generative Adversarial Networks (DCGANs)\ncomputer-vision\ndeep-learning\nDCGAN (Deep Convolutional General Adversarial Networks) uses convolutional layers in its design.\nAug 15, 2020\nGeneral Adversarial Networks (GANs)\ncomputer-vision\ndeep-learning\n\u201c\nGeneral Adversarial Nets\nis the most interesting idea in the last 10 years in machine learning\u201d. This was the statement from Yann LeCun regarding GANs when Ian Goodfellow\u2026\nAug 4, 2020\nPaper Explanation: Going deeper with Convolutions (GoogLeNet)\ncomputer-vision\ndeep-learning\nGoogle proposed a deep Convolution Neural Network named inception that achieved top results for classification and detection in ILSVRC 2014.\nJun 5, 2020\nVGGNet Architecture Explained\ncomputer-vision\ndeep-learning\nVGGNet is a Convolutional Neural Network architecture proposed by Karen Simonyan and Andrew Zisserman of University of Oxford in 2014. This paper mailny focuses in the\u2026\nMay 9, 2020\nAlexNet Architecture Explained\ncomputer-vision\ndeep-learning\nAlexNet famously won the 2012 ImageNet LSVRC-2012 competition by a large margin (15.3% vs 26.2%(second place) error rates). Here is the link to original paper.\nApr 24, 2020\nNo matching items"
      ]
    },
    {
      "title": "Prabin Nepal",
      "url": "https://nepalprabin.github.io/index.html",
      "relevance_score": 0.12042945623397827,
      "content": []
    },
    {
      "title": "nepalprabin (Prabin Nepal) \u00b7 GitHub",
      "url": "https://github.com/nepalprabin",
      "relevance_score": 0.11092301458120346,
      "content": [
        "nepalprabin (Prabin Nepal) \u00b7 GitHub\nSkip to content\nYou signed in with another tab or window.\nReload\nto refresh your session.\nYou signed out in another tab or window.\nReload\nto refresh your session.\nYou switched accounts on another tab or window.\nReload\nto refresh your session.\nDismiss alert\nnepalprabin\nFollow\nMore\nOverview\nRepositories\nProjects\nPackages\nStars\nnepalprabin\nFollow\n\ud83c\udfaf\nFocusing\nPrabin Nepal\nnepalprabin\n\ud83c\udfaf\nFocusing\nFollow\nSoftware Developer. AI enthusiast\n5\nfollowers\n\u00b7\n11\nfollowing\nUSA\nAchievements\nx2\nAchievements\nx2\nBlock or Report\nBlock or report nepalprabin\nBlock user\nPrevent this user from interacting with your repositories and sending you notifications.\nLearn more about\nblocking users\n.\nYou must be logged in to block users.\nAdd an optional note:\nPlease don't include any personal information such as legal names or email addresses. Maximum 100 characters, markdown supported. This note will be visible to only you.\nBlock user\nReport abuse\nContact GitHub support about this user\u2019s behavior.\nLearn more about\nreporting abuse\n.\nReport abuse\nMore\nOverview\nRepositories\nProjects\nPackages\nStars\nnepalprabin\n/\nREADME\n.md\nHi \ud83d\udc4b, I'm Prabin Nepal\nA passionate software developer\n\ud83c\udf31 I\u2019m currently learning\nDeep Learning for Computer Vision and NLP\n\ud83d\udcac Ask me about\nFull Stack Development, Deep Learning\n\ud83d\udceb How to reach me\nprabinnepal1996@gmail.com\nPinned\nLoading\noswrite\noswrite\nPublic\nPython\n2\nbasic-deeplearning-notebooks\nbasic-deeplearning-notebooks\nPublic\nJupyter Notebook\ndeeplearning-paper-implementation\ndeeplearning-paper-implementation\nPublic\nJupyter Notebook\n3\n1\nwhisper-webapp\nwhisper-webapp\nPublic\nJupyter Notebook\nSomething went wrong, please refresh the page to try again.\nIf the problem persists, check the\nGitHub status page\nor\ncontact support\n.\nYou can\u2019t perform that action at this time."
      ]
    }
  ]
}